<h1>Constitutional AI: Harmlessness from AI Feedback</h1><h2>In Layman English</h2><h4>Abstract</h4><p>  AI systems are getting better at helping us and supervising other AIs. We conducted experiments to train a helpful AI assistant using self-improvement methods without human labels for identifying harmful outputs. Instead, we gave the AI a set of rules or principles to follow. This method is called 'Constitutional AI'. The training process has two phases: supervised learning and reinforcement learning. In the supervised phase, we start with an initial model, generate self-critiques and revisions, and fine-tune the model based on the revised responses. In the RL phase, we compare two samples from the fine-tuned model using another model to evaluate which one is better. We then create a preference model from this data and use it as a reward signal for RL training, which we call 'RL from AI Feedback' (RLAIF). Through this process, we successfully trained a harmless AI assistant that handles harmful queries by explaining its objections. Both the supervised learning and reinforcement learning methods improve AI decision making and make it easier to control AI behavior with fewer human labels.</p><h4>Policy Memo</h4><p>  For more detailed information about Constitutional AI, you can read the <a href="https://cdn2.assets-servd.host/anthropic-website/production/images/Anthropic_ConstitutionalAI_v2.pdf">Constitutional AI Policy Memo</a>.</p>