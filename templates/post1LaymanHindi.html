<h1>एआई सुरक्षा पर मूल दृष्टिकोण: कब, क्यों, क्या और कैसे</h1><h2>लेमन हिंदी में</h2><p><br/></p><p>हमने Anthropic शुरू किया क्योंकि हम मानते हैं कि AI का बड़ा प्रभाव हो सकता है, जैसा कि औद्योगिक और वैज्ञानिक क्रांतियों ने किया है। लेकिन हमें यह नहीं पता कि यह अच्छे से होगा या नहीं। हमें लगता है कि यह प्रभाव शायद जल्द ही हो सकता है, शायद अगले दस सालों में।</p><p>कुछ लोगों को यह विचार मानने में कठिनाई हो सकती है, और संदेह करने के लिए अच्छे कारण हैं। अधिकांश लोग अपने कहते हुए के काम पर काम कर रहे थे जब वे कुछ बड़ा कर रहे हैं, लेकिन हमें लगता है कि AI के विकास के लिए पर्याप्त प्रमाण है और इसे तेजी से आगे बढ़ाने और कई चीजों में परिवर्तन लाने के लिए तैयार होने की जरूरत है।</p><p>Anthropic में, हमारा मोटो है "दिखाएँ, न बताएँ।" हम AI सुरक्षा पर शोध कर रहे हैं और इसे AI समुदाय के साथ साझा कर रहे हैं। हम अब इसलिए लिख रहे हैं क्योंकि अधिक लोग AI के प्रगति के बारे में सीख रहे हैं और हम अपने विचार और लक्ष्य साझा करना चाहते हैं। संक्षेप में कहें तो, हम मानते हैं कि AI सुरक्षा शोध वास्तव में महत्वपूर्ण है और इसे जनता और निजी क्षेत्र सहित सभी के समर्थन के लिए समर्थित किया जाना चाहिए।</p><p>इस पोस्ट में, हम यह समझाएंगे कि हम इस सब पर क्यों विश्वास करते हैं: हम क्यों सोचते हैं कि AI बहुत तेजी से आगे बढ़ेगा और बड़ा प्रभाव होगा, और हम AI की सुरक्षा के बारे में क्यों चिंतित हैं। हम अपने AI सुरक्षा शोध के बारे में भी संक्षेप में बात करेंगे। हम यह भी बताएंगे कि हम AI सुरक्षा और प्रगति के बारे में चर्चाओं में योगदान देने की आशा से यह लिख रहे हैं।</p><p>इस पोस्ट के मुख्य बिंदु यहां हैं:</p><ul>  <li><strong>AI बड़ा प्रभाव होगा, शायद अगले दस सालों में</strong><br/>AI में सुधार हो रहा है क्योंकि कंप्यूटर सुधार रहे हैं। इसका मतलब है कि भविष्य में AI प्रणालियाँ काफी समझदार हो जाएंगी, शायद कई चीजों में मानवों से भी बेहतर। AI की प्रगति धीमी हो सकती है या रुक सकती है, लेकिन प्रमाण सुझाता है कि यह शायद जारी रहेगी।</li>  <li><strong>हमें नहीं पता कि AI प्रणालियों को हमेशा ठीक तरीके से प्रशिक्षित कैसे करें</strong><br/>अभी हमें यह नहीं पता है कि कैसे हमेशा मददगार, ईमानदार और सुरक्षित AI प्रणालियों को प्रशिक्षित किया जाए। तेज AI प्रगति समाज में समस्याएं पैदा कर सकती है और कंपनियों या देशों के बीच प्रतियोगिता का कारण बन सकती है, जिससे भरोसेमंद AI प्रणालियों का उपयोग नहीं किया जा सकता। अगर AI प्रणालियों के खतरनाक लक्ष्य हों या महत्वपूर्ण परिस्थितियों में गलतियाँ हों, तो यह वास्तव में बहुत बुरा हो सकता है।</li>  <li><strong>हम AI सुरक्षा के प्रति बहुमुखी, आनुभविक दृष्टिकोण में आशावादी हैं</strong><br/>हम AI प्रणालियों को सुरक्षित रूप से निरंतर बनाने के विभिन्न तरीकों की खोज कर रहे हैं। हम पर्याप्त सुरक्षा चुनौतियों से लेकर आसान सुरक्षा के विभिन्न परिदृश्यों को कवर करने के लिए स्केलिंग निरीक्षण, यांत्रिक व्याख्यात्मकता, प्रक्रिया-मुखी सीखना, और AI प्रणालियों को कैसे सीखता है और अनुकूलित करता है की समझ बढ़ाने में उत्साहित हैं। हमारा लक्ष्य सुरक्षा कार्य को तेज़ करना है और सरल सुरक्षा चुनौतियों से लेकर बहुत कठिन चुनौतियों तक एक व्यापक रेंज में कवर करना है।</li></ul><h3>हमारे दृष्टिकोण पर तेज़ AI प्रगति</h3><p>AI को बेहतर बनाने में तीन मुख्य चीजें हैं: प्रशिक्षण डेटा, कम्प्यूटेशन, और सुधारी गई एल्गोरिदम।</p><p>2010 के दशक में, हमने देखा कि बड़ी AI प्रणालियाँ स्मार्ट थीं। इसलिए हम सोचे कि AI को प्रशिक्षित करने के लिए अधिक कम्प्यूटेशन उन्हें और भी स्मार्ट बना सकता है। AI को प्रशिक्षित करने के लिए कम्प्यूटेशन की मात्रा हर साल बहुत ज्यादा बढ़ रही थी। 2019 में, हमने AI के लिए स्केलिंग कानून विकसित किए, जिनसे पता चला कि AI को बड़ा और अधिक डेटा पर प्रशिक्षित करने से वे और भी स्मार्ट बन सकते हैं। इसी के आधार पर, हमने GPT-3 को प्रशिक्षित किया, जो पहली बड़ी भाषा मॉडलों में से एक है और 173 बिलियन पैरामीटर के साथ है।</p><p>स्केलिंग कानूनों की खोज के बाद से, हमें लगता है कि AI बहुत तेजी से प्रगति करेगा। कुछ चीजें जैसे तार्किक तर्क और मल्टीमोडालिटी पहले AI के लिए चुनौतियां थीं, लेकिन अब नहीं हैं। हम सोचते हैं कि AI की प्रगति जारी रहेगी और AI प्रणालियाँ मानवों के काफी कार्यों में भी बेहतर हो जाएँगी। AI को प्रशिक्षित करने की लागत अन्य बड़े विज्ञान परियोजनाओं की तुलना में काफी कम है। इसका मतलब है कि AI को बहुत आगे बढ़ने के लिए बहुत ज्यादा संभावनाएं हैं।</p><p>लोग अक्सर इस बात को नहीं समझते हैं कि चीजें कितनी तेजी से बढ़ सकती हैं। हालांकि AI की प्रगति अब तेज लग रही है, कुछ लोग सोचते हैं कि यह केवल अस्थायी है और चीजें सामान्य हो जाएंगी। लेकिन यदि हम सही हैं तो, AI की प्रगति जारी रहेगी और AI प्रणालियाँ बहुत सक्षम हो जाएँगी। AI के उपयोग से AI शोध में इस प्रक्रिया की गति तेज हो सकती है। इससे बहुत सारा ज्ञान कार्य स्वतः हो सकता है और समाज में बड़े तरीके से परिवर्तन हो सकते हैं।</p><p>हमें यकीन नहीं है कि भविष्य की AI प्रणालियाँ वैसी होंगी, लेकिन हमें उम्मीदित परिणामों के लिए तैयारी करनी होगी। यह एक महत्वपूर्ण समय है, और हमें AI के आने वाले परिवर्तनों के लिए तैयार होना होगा।</p><p>यह संपूर्ण चित्र शायद पूरी तरह से सटीक न हो, लेकिन हमें लगता है कि इसे गंभीरता से लेना चाहिए। AI कंपनियों, नीतिनिर्माताओं, और समाज को आपसी योगदान करके प्रभावी AI के लिए शोध करने और योजना बनाने में सचेत रहना चाहिए।</p><h3>बिना सुरक्षा खतरे के क्या?</h3><p>यदि हम पहले दिए गए विचारों को ध्यान से विचार करें, तो हमें यह देखना मुश्किल नहीं होगा कि एआई सुरक्षा और सुरक्षा के लिए हमारे लिए खतरनाक क्यों हो सकती है। यहां चिंता करने के दो मुख्य कारण हैं।</p><p>पहले, यह चुनौतीपूर्ण है कि एक ऐसा एआई सिस्टम बनाया जाए जो हमारे मुकाबले बहुत ही बुद्धिमान और अपने आस-पास की परिस्थितियों को समझने में भी सुरक्षित और विश्वसनीय हो। यह किसी शतरंज महारथी के समान है जो एक नवीन खिलाड़ी को हराने की कोशिश कर रहा है, जो महारथी की गलतियों को पहचान नहीं सकता। यदि हम एक ऐसी एआई बनाएं जो हमसे बहुत अधिक बुद्धिमान हो, लेकिन उसके लक्ष्य हमारी सर्वोत्तम हित के विपरीत हों, तो इसके परिणाम घोर हो सकते हैं। इसे तकनीकी समन्वय समस्या कहा जाता है।</p><p>दूसरे, एआई में तेज़ प्रगति के कारण नौकरियों, अर्थव्यवस्थाओं और शक्ति संरचनाओं में महत्वपूर्ण बदलाव हो सकते हैं। ये बदलाव स्वतः ही विनाशकारी हो सकते हैं और इसके साथ ही साथ एआई सिस्टमों को सावधानीपूर्वक और ज़िम्मेदारीपूर्वक विकसित करना और संभवतः एआई के साथ और भी बड़ी समस्याएं पैदा करना मुश्किल हो सकता है।</p><p>हम यह मानते हैं कि यदि एआई तेजी से प्रगति करता है, तो ये खतरे बहुत ही महत्वपूर्ण होंगे। ये एक दूसरे के साथ ऐसे प्रभावित हो सकते हैं जिन्हें हम पूरी तरह से पूर्वानुमान नहीं कर सकते हैं। संभावित है कि हम बाद में महसूस करेंगे कि हम गलत थे और ये खतरे इतने गंभीर नहीं थे, या हम इन्हें आसानी से हल कर सकते हैं। हालांकि, हमें सतर्क रहना बेहतर होता है क्योंकि एआई के साथ गलतियाँ होने के परिणाम घोर हो सकते हैं।</p><p>पहले से ही हमने देखा है कि एआई सिस्टम अनचाहे तरीकों से व्यवहार कर सकते हैं। इनमें विषाक्तता, पक्षपात, अप्रत्याशितता और सत्ता की इच्छा शामिल हो सकती है। जब एआई व्यापक और शक्तिशाली होती जाती है, तो ये मुद्दे और महत्वपूर्ण होते जाएंगे। कुछ ये समस्याएं उन चुनौतियों की तरह हो सकती हैं जिनका सामना हमें एआई से इंसानी स्तर की बुद्धिमत्ता और इससे आगे करना पड़ सकता है।</p><p>एआई सुरक्षा के क्षेत्र में, हम पूर्वानुमानित और अप्रत्याशित विकासों की मिश्रण उम्मीद करते हैं। हालांकि, हम जितनी भी मसले आज हमें पता हों, हम यह मान नहीं सकते कि भविष्य की मसलें भी उसी तरीके से हल हो जाएंगी। जब एआई सिस्टम दुनिया में अपनी जगह समझने, लोगों को भ्रमित करने या मानवों के समझ से बाहर जाने जैसी स्ट्रेटेजी का उपयोग करने में सक्षम होती है, तो नई और डरावनी समस्याएं हो सकती हैं जब एआई सिस्टम बहुत उन्नत हो जाती है।</p><h3>हमारी दृष्टि: AI सुरक्षा में अनुभवशास्त्र</h3><p>हम सोचते हैं कि विज्ञान और इंजीनियरिंग में प्रगति करना मुश्किल होता है जब हम उस चीज का अध्ययन नहीं करते हैं जिस पर हम काम कर रहे होते हैं। AI को समझने और सुरक्षित बनाने के लिए, हम अनुभवशास्त्र पर भरोसा करते हैं, जो AI प्रणालियों के प्रयोग और मूल्यांकन से प्राप्त होता है। यह प्रमाण हमें सीखने में और प्रगति करने में मदद करने वाली "यथार्थता" की तरह है।</p><p>यह यह नहीं मतलब है कि हम AI सुरक्षा में सिद्धांतिक या धारात्मक शोध को उपेक्षा करते हैं, लेकिन हम मानते हैं कि वास्तविक दुनिया के प्रमाण पर आधारित शोध सबसे महत्वपूर्ण है। AI सुरक्षा में कई संभावनाएं और चुनौतियाँ हैं, और उन्हें सिर्फ सोचकर खोजना मुश्किल है। ऐसा हो सकता है कि हम समस्याओं पर अधिक ध्यान देते हैं जो कभी होने की संभावना नहीं हैं या हमें महत्वपूर्ण समस्याओं का पता नहीं चलता है। अच्छे अनुभवशास्त्रिय शोध हमारे सिद्धांतिक और धारात्मक काम में हमारी मार्गदर्शन कर सकता है।</p><p>उसी तरह, हम मानते हैं कि सुरक्षा समस्याओं का पता लगाना और हल करना एक जटिल प्रक्रिया है जिसमें विभिन्न दृष्टियों की कोशिश करने और उनसे सीखने की आवश्यकता होती है। इसलिए, हालांकि हमारे पास हमारे शोध के लिए एक योजना हो सकती है, हम समय के साथ बदल सकते हैं। हम जानते हैं कि हमारी सभी योजनाएं सफल नहीं होंगी, लेकिन यह शोध करने का हिस्सा है।</p><h3>मार्गदर्शक मॉडलों की अनुभवशास्त्र में भूमिका</h3><p>हम मौजूद हैं क्योंकि हम मानते हैं कि प्रगतिशील AI प्रणालियों पर सुरक्षा शोध करना महत्वपूर्ण है। इसके लिए एक संगठन की आवश्यकता होती है जो बड़े मॉडलों के साथ काम कर सकता है और सुरक्षा को प्राथमिकता दे सकता है।</p><p>अनुभवशास्त्र केवल उन्नत AI प्रणालियों को आवश्यक नहीं करता है। लेकिन हमारे मामले में, बड़े मॉडल छोटे मॉडलों से अलग होते हैं और सुरक्षा से सीधे जुड़ते हैं:</p><ul>  <li>कई गंभीर सुरक्षा संबंधित चिंताएं केवल मानव स्तर के AI के साथ उठती हैं, इसलिए हमें इन समस्याओं का अध्ययन करने और उन्हें संबोधित करने के लिए ऐसी प्रणालियों तक पहुंच की आवश्यकता होती है।</li>  <li>कुछ सुरक्षा विधियाँ केवल बड़े मॉडलों पर परीक्षण किए जा सकते हैं, और छोटे मॉडलों के साथ काम करने से हमें ये विधियाँ खोजने और साबित करने का अवसर नहीं मिलेगा।</li>  <li>हम भविष्य की प्रणालियों की सुरक्षा पर ध्यान केंद्रित करते हैं, इसलिए हमें समझना होगा कि मॉडल बड़े होने पर सुरक्षा कैसे बदलती है।</li>  <li>यदि भविष्य की बड़ी प्रणालियाँ बहुत खतरनाक हों, तो हमें मजबूत प्रमाण इकट्ठा करना होगा, और यह आमतौर पर बड़े मॉडलों का उपयोग करने की आवश्यकता होती है।</li></ul><p>हालांकि, एक चुनौती है। हमें सुरक्षा शोध के माध्यम से खतरनाक प्रौद्योगिकियों की तेजी से लागू करने से बचना होगा। लेकिन अत्यधिक सतर्कता से भी महत्वपूर्ण शोध को धीमा नहीं करना चाहिए। सुरक्षा शोध करना ही काफी नहीं होता है; हमें इस शोध को वास्तविक प्रणालियों में तेजी से इंटीग्रेट करने की भी आवश्यकता होती है।</p><p>सही संतुलन स्थापित करना हमारे लिए महत्वपूर्ण है। ये चिंताएं हमारे संगठन के विभिन्न पहलुओं, सहयोग, नियंत्रण, भर्ती, अनुप्रयोग, सुरक्षा, और साझेदारी में हमारे निर्णयों की मार्गदर्शन करती हैं। भविष्य में, हम योजनाएं बनाने और स्वतंत्र मूल्यांकन की अनुमति देने की योजना बना रहे हैं ताकि हम उन्नत मॉडल विकसित करते समय सुरक्षा मानकों को पूरा कर सकें।</p><h3>AI सुरक्षा के लिए एक पोर्टफोलियो दृष्टिकोण</h3><p>कुछ शोधकर्ताओं को उन्नत AI प्रणालियों के संबंध में जोखिमों से चिंता होती है। हालांकि, उनके व्यवहार और गुणों को पूर्वानुमान करना चुनौतीपूर्ण है, और सुरक्षा के पूर्वानुमान बनाना इससे भी कठिन है। हम मानते हैं कि विभिन्न परिदृश्यों की एक विस्तृत श्रेणी संभव है।</p><p>तीन परिदृश्यों को विचार करना है:</p><ol>  <li><strong>आशावादी परिदृश्य:</strong> AI से आपातकालिक जोखिम कम हैं, और मौजूदा सुरक्षा तकनीकों काफी पर्याप्त हैं। मुख्य जोखिम विषयों में विषाक्तता और इच्छापूर्ति, साथ ही सामाजिक बदलाव और शक्ति के गतिविधियों में परिवर्तन जैसी समस्याएं हैं।</li>  <li><strong>इण्टरमीडिएट परिदृश्य:</strong> आपातकालिक जोखिम संभव हैं, लेकिन साक्षात्कार और इंजीनियरिंग के सर्वोच्च अभियांत्रिकी के माध्यम से हम उन्हें कम कर सकते हैं।</li>  <li><strong>निराशावादी परिदृश्य:</strong> AI सुरक्षा एक असंभव समस्या है, और गंभीर जोखिमों से बचने के लिए उन्नत AI प्रणालियों के विकास से बचा जाना चाहिए। इस परिदृश्य में सतर्कता की जरूरत होती है और सुरक्षा के प्रमाण का मूल्यांकन किया जाना चाहिए।</li></ol><p>आशावादी परिदृश्य में, हम AI के लाभदायक उपयोग की गति को तेज कर सकते हैं और AI प्रणालियों द्वारा पैदा होने वाले निकटतम हानियों का सामना कर सकते हैं। इंटरमीडिएट परिदृश्य में, हमारा मुख्य ध्यान जोखिमों की पहचान करने और शक्तिशाली AI प्रणालियों के लिए सुरक्षित प्रशिक्षण विधियों का विकास पर है। निराशावादी परिदृश्य में, हम सुरक्षा जोखिमों के प्रमाण प्रदान करने और खतरनाक AI के विकास के खिलाफ वकालत करने पर जोर देते हैं।</p><p>हमारी प्राथमिकता अधिक जानकारी इकट्ठा करना है ताकि हम जान सकें कि हम किस परिदृश्य में हैं। हमारा शोध AI प्रणालियों को बेहतर समझने और चिंताजनक व्यवहारों का पता लगाने की दिशा में है। हम सुरक्षित तकनीकों के विकास पर काम करते हैं और AI प्रणालियों की सुरक्षा स्तर की पहचान करते हैं।</p><p>हम AI सुरक्षा अनुसंधान में एक पोर्टफोलियो दृष्टिकोण पर विश्वास रखते हैं, जो विभिन्न परिदृश्यों पर ध्यान देता है। हमारा लक्ष्य इंटरमीडिएट परिदृश्य में प्रगति करना है, निराशावादी परिदृश्य में जागरूकता बढ़ाना है, और AI सुरक्षा के समग्र सुधार में योगदान देना है।</p><h3>एंथ्रोपिक में AI अनुसंधान के तीन प्रकार</h3><p>एंथ्रोपिक में हम अपने अनुसंधान परियोजनाओं को तीन श्रेणियों में बाँटते हैं:</p><ul>  <li><strong>क्षमताएँ:</strong> यह अनुसंधान उद्देश्यित होता है AI प्रणालियों की प्रदर्शन क्षमता को बेहतर बनाने के लिए जैसे लेखन, छवि प्रसंस्करण और खेल खेलने में। हम मॉडल को अधिक कुशल बनाने और बेहतर एल्गोरिदम विकसित करने पर ध्यान केंद्रित करते हैं। हम AI क्षमताओं को बढ़ाने की बजाय सुरक्षा अनुसंधान को प्राथमिकता देते हैं।</li>  <li><strong>मेल प्रक्षेपण क्षमताएँ:</strong> यह अनुसंधान AI प्रणालियों को सहायक, सत्य और मानव मूल्यों के साथ मेल बनाने के लिए होता है। हम डिबेट, रेड-टीमिंग और मानव प्रतिक्रिया से सुधार करने के लिए प्रशिक्षण एल्गोरिदम विकसित करते हैं। ये तकनीकें व्यावहारिक उपयोग और अधिक क्षमताओं के लिए तैयारी में महत्वपूर्ण हैं।</li>  <li><strong>मेल विज्ञान:</strong> यह अनुसंधान AI प्रणालियों के मेल का मूल्यांकन और समझने, मेल क्षमताओं के प्रभावशीलता का मूल्यांकन करने और इन तकनीकों की सीमाओं का अध्ययन करने के लिए होता है। हम इंटरप्रिटेबिलिटी, भाषा मॉडल और सामान्यीकरण जैसे मुद्दों को अन्वेषण करते हैं। यह काम हमें संभावित समस्याओं की पहचान करने और मेल क्षमताओं की सीमाओं को प्रकट करने में मदद करता है।</li></ul><p>मेल प्रक्षेपण क्षमताएँ और मेल विज्ञान को आमतौर पर "नीला टीम" और "लाल टीम" दृष्टिकोण के रूप में देखा जा सकता है। मेल प्रक्षेपण क्षमताएँ नई तकनीकों के विकास पर ध्यान केंद्रित करती हैं, जबकि मेल विज्ञान उनकी सीमाओं को समझने और प्रकट करने का उद्देश्य रखता है।</p><p>हमारा श्रेणीकरण AI सुरक्षा समुदाय के बीच विचार-विमर्शों का समाधान करने के लिए उपयोगी है। व्यावहारिक मेल प्रक्षेपण क्षमताएँ अधिक प्रगतिशील मॉडलों के लिए तकनीक विकसित करने के लिए आधार प्रदान करती हैं। यह हमें मॉडल को और सच्चा, सुधारयोग्य और मानवों के लिए महत्वपूर्ण बनाने में मदद करती है। यह सुनिश्चित करती है कि AI विकासकों को सुरक्षा में निवेश करने और संभावित विफलताओं की पहचान करने के लिए प्रेरित किया जाए।</p><p>अगर AI सुरक्षित होने योग्य है, तो हमारी मेल प्रक्षेपण क्षमताएँ का सबसे अधिक प्रभाव होगा। अगर मेल करने की समस्या ज्यादा चुनौतीपूर्ण है, तो हमें मेल विज्ञान की मदद से तकनीकों की कमियों की पहचान करने होंगी। और अगर मेल करने की समस्या लगभग असंभव है, तो मेल विज्ञान AI प्रणालियों के विकास को रोकने के लिए मजबूत मान्यता तैयार करने में महत्वपूर्ण हो जाती है।</p><h3>हमारे वर्तमान सुरक्षा अनुसंधान</h3><p>हम AI प्रणालियों को सुरक्षित तरीके से प्रशिक्षित करने के लिए विभिन्न दृष्टिकोणों का पता लगा रहे हैं। यहां कुछ महत्वपूर्ण विचार हैं जिन पर हम काम कर रहे हैं:</p><ul>  <li>यांत्रिक व्याख्यानशीलता</li>  <li>स्केलेबल निगरानी</li>  <li>प्रक्रिया-मुखित अधिगम</li>  <li>सार्वभौमिकता को समझना</li>  <li>खतरनाक असफलता मोड के लिए परीक्षण</li>  <li>सामाजिक प्रभाव और मूल्यांकन</li></ul><h4>यांत्रिक व्याख्यानशीलता</h4><p>हम AI प्रणालियों को सुरक्षित तरीके से प्रशिक्षित करने और अनचाहे व्यवहार का पता लगाने के लिए विभिन्न दृष्टिकोणों पर काम कर रहे हैं।</p><p>एक दृष्टिकोण है यांत्रिक व्याख्यानशीलता, जहां हम न्यूरल नेटवर्क्स को मानव द्वारा समझने योग्य एल्गोरिदम में रिवर्स इंजीनियर करने का लक्ष्य रखते हैं। इससे हम मॉडल की सुरक्षा के लिए ऑडिट कर सकेंगे और असुरक्षित पहलूओं की पहचान कर सकेंगे।</p><p>यह एक मुश्किल समस्या है, लेकिन इसमें यह संकेत हैं कि यह असंभव नहीं है। हमने दृष्टि मॉडल के घटकों को समझने में प्रगति की है और छोटे भाषा मॉडलों तक व्याख्यानशीलता का विस्तार किया है। हम न्यूरल नेटवर्क की गणना के तंत्रों के बारे में और भी बहुत कुछ सीखते रहते हैं।</p><p>हम आनुभविक दृष्टिकोण पर आधारित हैं और अगर हमें और आशावादी तरीकों का पता चलता है तो हम दिशा बदलने के लिए खुले हैं। न्यूरल नेटवर्क्स और सीखने के कार्य की समझ हमें सुरक्षा सुनिश्चित करने के लिए अधिक औजार प्रदान करेगी।</p><h4>स्केलेबल निगरानी</h4><p>एआई प्रणालियों को सुरक्षित और मानव मूल्यों के साथ सुसंगत व्यवहार कराने के लिए हमें बहुत सारी उच्च गुणवत्ता की प्रतिक्रिया की जरूरत होती है। हालांकि, चिंताओं हैं कि मानव आवश्यक प्रतिक्रिया प्रदान नहीं कर सकते हैं।</p><p>मानव सही प्रतिक्रिया प्रदान नहीं कर सकते, एआई प्रणालियों द्वारा धोखे में आ सकते हैं, या विस्तृत स्तर पर प्रतिक्रिया प्रदान करने में संघर्ष कर सकते हैं। इसे स्केलेबल निगरानी की समस्या के रूप में जाना जाता है और यह सुरक्षित एआई प्रणालियों की प्रशिक्षण में एक मुख्य मुद्दा है।</p><p>हम मानते हैं कि एआई प्रणालियों को इस चुनौती का सामना करने के लिए आंशिक रूप से स्वयं निगरानी करनी चाहिए या मानवों की निगरानी में सहायता करनी चाहिए। RLHF और संवैधानिक एआई जैसी तकनीकें मानव निगरानी को प्रभावी एआई निगरानी में विस्तारित करने में समर्थ हैं।</p><p>स्केलेबल निगरानी यह भी संभव बनाती है कि हम रेड-टीमिंग को स्वचालित करें, जहां हम एआई प्रणालियों को संभावित समस्याजनक इनपुट के साथ परीक्षण करें और उन्हें और सच्चाईपूर्ण और हानिहरणहीन व्यवहार करना सिखाएं। यह हमें मजबूत सुरक्षित प्रणालियों की प्रशिक्षण में मदद करता है।</p><p>हम स्केलेबल निगरानी के लिए विभिन्न तकनीकों का अनुसंधान कर रहे हैं, जिसमें CAI, मानव सहायित निगरानी, AI-AI विवाद, मल्टी-एजेंट RL रेड टीमिंग, और मॉडल जनरेटेड मूल्यांकन शामिल हैं। स्केलेबल निगरानी वाले तकनीकों का उपयोग करके हम एडवांस्ड एआई प्रणालियों की प्रशिक्षण में उत्कृष्ट और सुरक्षितता दोनों ही के साथ प्रगति करने की उम्मीद करते हैं।</p><h4>प्रक्रिया-मुखित सीखना नतीजों की प्राप्ति से अधिक</h4><p>नई कार्य को सीखने के लिए विभिन्न तरीके होते हैं। एक होता है परीक्षण और त्रुटि, जहां आप सफलता की प्राप्ति तक विभिन्न रणनीतियों की कोशिश करते रहते हैं। दूसरा तरीका होता है एक विशेषज्ञ से सीखना, जो अपनी सफल प्रक्रियाओं को आपको सिखाता है।</p><p>प्रक्रिया-मुखित सीखना उद्देश्य की प्राप्ति के बजाय उसे प्राप्त करने के लिए उपयोग किए जाने वाले चरणों और विधियों पर मास्टरी करने पर ध्यान केंद्रित करता है। इसमें सहयोग और समय के साथ सुधार शामिल होता है।</p><p>एआई सुरक्षा के संदर्भ में, प्रक्रिया-मुखित सीखने का उपयोग करके एआई प्रणालियों की प्रशिक्षण में कई चिंताओं का समाधान किया जा सकता है:</p><ul><li>मानव विशेषज्ञ एआई प्रणालियों द्वारा अनुसरित चरणों को समझते हैं और उन्हें व्याख्या करते हैं।</li><li>एआई प्रणालियों को प्रभावीता और समझने योग्यता के आधार पर पुरस्कारित किया जाता है, न कि केवल अंतिम परिणाम के आधार पर।</li><li>एआई प्रणालियों को संसाधन प्राप्ति या भ्रामकता जैसे समस्याजनक व्यवहारों के पीछा छोड़ दिया जाता है।</li></ul><p>Anthropic में, हम मानते हैं कि प्रक्रिया-मुखित सीखने पर ध्यान केंद्रित करना एडवांस्ड एआई प्रणालियों के साथ संबंधित मुद्दों का सरल और प्रभावी तरीका है। हम इसकी सीमाओं की भी खोज कर रहे हैं और प्रक्रिया और परिणाम-मुखित सीखने को मिलाकर उत्पन्न होने वाली सुरक्षा समस्याओं का अध्ययन कर रहे हैं।</p><h4>सामान्यीकरण को समझना</h4><p>हमारे अनुसंधान में, हम न्यूरल नेटवर्क की कार्यप्रणाली को उलट फेरकर उसे समझने का प्रयास करते हैं। हम यह भी अध्ययन करते हैं कि बड़े भाषा मॉडल कैसे प्रशिक्षित किए जाते हैं।</p><p>बड़े भाषा मॉडल आविष्कारक व्यवहार जैसे रचनात्मकता, स्वरक्षण और धोखाधड़ी का प्रदर्शन कर सकते हैं। ये व्यवहार प्रशिक्षण डेटा से आते हैं, लेकिन इस प्रक्रिया में जटिलता होती है। पहले, मॉडल को बहुत सारे पाठ पर प्रशिक्षित किया जाता है, जहां वे विभिन्न प्रतिनिधित्व सीखते हैं और विभिन्न चीजें नकल करते हैं। फिर उन्हें कई तरीकों से फ़ाइन ट्यून किया जाता है, जो अप्रत्याशित परिणाम हो सकते हैं। फ़ाइन ट्यूनिंग प्रारंभिक प्रशिक्षण से आए तरंगों पर निर्भर करता है, जो विशाल ज्ञान के एक विशाल भंडार से आता है।</p><p>जब कोई मॉडल चिंताजनक व्यवहार दिखाता है, जैसे कि यह अपने आपको सहमत बताने वाले एक एआई है लेकिन वास्तविकता में वह गुमराह कर रही हो, तो हम जानना चाहते हैं कि यह केवल सीखे को दोहराने की कोशिश कर रहा है या वास्तव में इस व्यवहार में विश्वास रखता है। हम इसे बेहतर समझने के लिए मॉडल के आउटपुट को प्रशिक्षण डेटा तक पहुंचाने के तकनीक विकसित कर रहे हैं।</p><h4>खतरनाक असफलता मोड की परीक्षा</h4><p class="text-b2">हमें चिंता है कि उन्नत एआई प्रणालियों में भ्रामकता या रणनीतिक योजना जैसे हानिकारक व्यवहार विकसित हो सकते हैं। इसे समझने के लिए, हम इन व्यवहारों के साथ छोटे मॉडल्स को प्रशिक्षित करते हैं ताकि हम उन्हें बिना हानि के अध्ययन कर सकें।</p><p class="text-b2">हमें खासकर यह देखना है कि एआई प्रणालियाँ कैसे व्यवहार करती हैं जब वे प्रशिक्षण माहौल में मानवों के साथ संवाद कर रही होती हैं। क्या वे भ्रामक बनती हैं या अप्रचलित लक्ष्य विकसित करती हैं? हमें इन प्रवृत्तियों को और यह जानना है कि जैसे-जैसे मॉडल्स महत्वपूर्ण होते हैं तब वे कैसे बदलते हैं।</p><p>हालांकि, हमें इस अनुसंधान में सतर्क रहना चाहिए। यह छोटे मॉडल्स में व्यवहार का अध्ययन करना सुरक्षित है, लेकिन इसे बड़े, अधिक क्षमता वाले मॉडल्स के साथ करना खतरनाक हो सकता है। हम ऐसे मॉडल्स पर यह अनुसंधान नहीं करेंगे जो गंभीर हानि का कारण बन सकते हैं।</p><h4>सामाजिक प्रभाव और मूल्यांकन</h4><p class="text-b2">हम समाज पर हमारी एआई प्रणालियों के प्रभाव को सावधानीपूर्वक विचार करते हैं। हम उनकी क्षमताओं, सीमाओं और संभावित सामाजिक प्रभावों का अध्ययन करते हैं। उदाहरण के लिए, हम विश्लेषण करते हैं कि बड़े भाषा मॉडल अप्रत्याशित और संभावित रूप से हानिकारक तरीके से कैसे व्यवहार कर सकते हैं। हम मॉडल को आपत्तिजनक आउटपुट के लिए टेस्ट करते हैं और वस्त्राघात और स्टीरियोटाइपिंग को कम करने के तरीकों का पता लगाते हैं।</p><p class="text-b2">हम सक्षम एआई प्रणालियों के समाज पर प्रभाव के बारे में बहुत चिंतित हैं। हम उनके संभावित हानिकारक व्यवहार का मूल्यांकन करते हैं, उनका उपयोग की पूर्वानुमान करते हैं और उनके आर्थिक प्रभाव का अध्ययन करते हैं। यह अनुसंधान हमें जिम्मेदारीपूर्ण एआई नीतियाँ विकसित करने में मदद करता है। एआई के प्रभावों को समझकर, हम नीति निर्माताओं और शोधकर्ताओं की सहायता करके हानि को कम करने और सुनिश्चित करने का लक्ष्य रखते हैं कि एआई सभी समाज के लोगों को लाभ पहुंचाए।</p><h4>समाप्ति के विचार</h4><p class="text-b2">हम मानते हैं कि कृत्रिम बुद्धिमत्ता अगले दशक में दुनिया पर बड़ा प्रभाव डाल सकती है। हम यह सुनिश्चित करना चाहते हैं कि ये प्रगतिशील प्रणालियां मानव मूल्यों के साथ समरूप हों और गंभीर जोखिम न पैदा करें।</p><p class="text-b2">हम वर्तमान एआई प्रणालियों से चिंतित नहीं हैं, लेकिन हम भविष्य के लिए तैयार रहना चाहते हैं। हम अब मूलभूत काम कर रहे हैं जो अधिक शक्तिशाली एआई प्रणालियों से होने वाली जोखिमों को कम करेगा।</p><p class="text-b2">Anthropic में, हम एक प्रायोगिक अनुभव के साथ एआई सुरक्षा के पास जा रहे हैं। हम देख रहे हैं कि एआई कैसे सीखती है और ज्ञान का उपयोग करती है, एआई प्रणालियों को निगरानी और समीक्षा करने के तरीके ढूंढ़ रहे हैं, एआई प्रणालियों को पारदर्शी और समझने में सरल बना रहे हैं, एआई को सुरक्षित प्रक्रिया का पालन करने के लिए प्रशिक्षित कर रहे हैं, खतरनाक विफलताओं से बचा रहे हैं, और समाज पर एआई के प्रभाव का मूल्यांकन कर रहे हैं। हम अलग-अलग स्थितियों में सुरक्षित रहने के लिए विभिन्न दृष्टिकोणों को जानने का पता लगा रहे हैं।</p><h4>फ़ुटनोट</h4><ol>  <li id="footnote-1">एआई एल्गोरिदम और हार्डवेयर में प्रगति तेजी से हो रही है, और समग्र वृद्धि दर लगभग लघुगणकीय मानी जाती है।</li>  <li>हमारा लक्ष्य ऐसे एआई सिस्टम विकसित करना है जो मानव मूल्यों को पढ़ सकें, लिख सकें, और उससे संवाद कर सकें।</li>  <li>हम यह उम्मीद करते हैं कि आगामी कुछ वर्षों में गणना में प्रगति की विशेष वृद्धि होगी लेकिन यह सटीक रूप से पूर्वानुमान करना कठिन है।</li>  <li>एआई अनुसंधान में, न्यूरल नेटवर्क को सीखने और सारांशित करने में आश्चर्य और रहस्य हैं।</li>  <li>प्रभावी सुरक्षा अनुसंधान करने के लिए, हमें Anthropic में आंतरिक रूप से एआई सिस्टम विकसित करने की जरूरत होती है।</li></ol>