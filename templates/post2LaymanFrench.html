<h1>Tracer un chemin vers la responsabilité de l'IA</h1><h2>En termes simples en français</h2><p><br/></p><p>Cette semaine, Anthropic a soumis une réponse à la demande de commentaires de la National Telecommunications and Information Administration (NTIA) sur la responsabilité de l'intelligence artificielle (IA). Aujourd'hui, nous voulons partager nos recommandations car elles capturent certaines des principales propositions politiques de l'IA chez Anthropic.</p><p>Il n'existe actuellement aucun processus solide et complet pour évaluer les systèmes d'intelligence artificielle (IA) avancés d'aujourd'hui, sans parler des systèmes plus performants de l'avenir. Notre soumission présente notre point de vue sur les processus et les infrastructures nécessaires pour garantir la responsabilité de l'IA. Nos recommandations envisagent le rôle potentiel de la NTIA en tant qu'organisme de coordination qui établit des normes en collaboration avec d'autres organismes gouvernementaux tels que le National Institute of Standards and Technology (NIST).</p><p>Dans nos recommandations, nous nous concentrons sur les mécanismes de responsabilité adaptés aux modèles d'IA hautement capables et polyvalents. Plus précisément, nous recommandons :</p><ul>  <li>    <strong>Financer la recherche pour améliorer les évaluations</strong>    <ul>      <li>Augmenter le financement de la recherche sur l'évaluation des modèles d'IA. Développer des évaluations rigoureuses et standardisées est un travail difficile et chronophage qui nécessite des ressources importantes. Un financement accru, notamment de la part des organismes gouvernementaux, pourrait contribuer à faire avancer cette zone critique.</li>      <li>Exiger des entreprises à court terme de divulguer leurs méthodes et résultats d'évaluation. Les entreprises déployant des systèmes d'IA devraient être tenues de satisfaire à certaines exigences en matière de divulgation concernant leurs évaluations, même si ces exigences ne doivent pas être rendues publiques si cela compromettrait la propriété intellectuelle (PI) ou les informations confidentielles. Cette transparence pourrait aider les chercheurs et les décideurs à mieux comprendre les lacunes des évaluations existantes.</li>      <li>Développer à long terme un ensemble de normes et de bonnes pratiques d'évaluation de l'industrie. Des organismes gouvernementaux tels que le NIST pourraient travailler à l'établissement de normes et de références pour évaluer les capacités, les limites et les risques des modèles d'IA auxquels les entreprises devraient se conformer.</li>    </ul>  </li>  <li>    <strong>Créer des évaluations adaptées aux risques en fonction des capacités des modèles</strong>    <ul>      <li>Développer des évaluations standardisées des capacités des systèmes d'IA. Les gouvernements devraient financer et participer au développement d'évaluations rigoureuses des capacités et de la sécurité, ciblées sur les risques critiques liés à l'IA avancée, tels que la tromperie et l'autonomie. Ces évaluations peuvent fournir une base factuelle pour une réglementation proportionnée et sensible aux risques.</li>      <li>Développer un seuil de risque grâce à davantage de recherche et de financement dans les évaluations de sécurité. Une fois qu'un seuil de risque aura été établi, nous pourrons exiger des évaluations pour tous les modèles par rapport à ce seuil.        <ul>          <li>Si un modèle tombe en dessous de ce seuil de risque, les normes de sécurité existantes sont probablement suffisantes. Vérifier la conformité et déployer.</li>          <li>Si un modèle dépasse le seuil de risque et que les évaluations de sécurité et les mesures d'atténuation sont insuffisantes, arrêter le déploiement, renforcer considérablement la surveillance et informer les régulateurs. Déterminer les mesures de sécurité appropriées avant d'autoriser le déploiement.</li>        </ul>      </li>    </ul>  </li>  <li>    <strong>Établir une pré-inscription pour les importantes exécutions de formation d'IA</strong>    <ul>      <li>Établir un processus pour que les développeurs d'IA signalent les importantes exécutions de formation afin que les régulateurs soient conscients des risques potentiels. Cela implique de déterminer le destinataire approprié, les informations requises et les mesures de cybersécurité, de confidentialité, de PI et de protection de la vie privée appropriées.</li>      <li>Établir un registre confidentiel pour les développeurs d'IA effectuant des importantes exécutions de formation, afin qu'ils enregistrent les détails du modèle auprès du gouvernement national de leur pays (par exemple, les spécifications du modèle, le type de modèle, l'infrastructure informatique, la date prévue de fin de la formation et les plans de sécurité) avant que la formation ne commence. Les données agrégées du registre doivent être protégées selon les normes et spécifications les plus élevées disponibles.</li>    </ul>  </li>  <li>    <strong>Donner du pouvoir aux auditeurs tiers qui sont...</strong>    <ul>      <li><strong>Techniquement compétents</strong> – certains auditeurs auront besoin d'une solide expérience en apprentissage automatique ;</li>      <li><strong>Soucieux de la sécurité</strong> – bien placés pour protéger la propriété intellectuelle précieuse, qui pourrait constituer une menace pour la sécurité nationale si elle était volée ; et</li>      <li><strong>Flexibles</strong> – capables de mener des évaluations solides mais légères qui détectent les menaces sans compromettre la compétitivité des États-Unis.</li>    </ul>  </li>  <li>    <strong>Exiger des tests de validation externe avant la publication des modèles</strong>    <ul>      <li>Exiger des tests de validation externe pour les systèmes d'IA, soit par l'intermédiaire d'un tiers centralisé (par exemple, le NIST), soit de manière décentralisée (par exemple, via l'accès à l'API des chercheurs), afin de normaliser les tests adverses des systèmes d'IA. Cela devrait être une condition préalable pour les développeurs qui publient des systèmes d'IA avancés.</li>      <li>Établir des options de tests de validation externe de haute qualité avant qu'elles ne deviennent une condition préalable à la publication des modèles. Ceci est crucial car les talents en tests de validation externe résident actuellement presque exclusivement dans des laboratoires privés d'IA.</li>    </ul>  </li>  <li>    <strong>Favoriser la recherche sur l'interprétabilité</strong>    <ul>      <li>Augmenter le financement de la recherche sur l'interprétabilité. Fournir des subventions gouvernementales et des incitations pour les travaux d'interprétabilité dans les universités, les organisations à but non lucratif et les entreprises. Cela permettrait de réaliser des travaux significatifs sur des modèles plus petits, favorisant ainsi les progrès en dehors des laboratoires de pointe.</li>      <li>Reconnaître que les réglementations exigeant des modèles interprétables seraient actuellement impossibles à satisfaire, mais pourraient être possibles à l'avenir grâce aux avancées de la recherche.</li>    </ul>  </li>  <li>    <strong>Permettre la collaboration industrielle sur la sécurité de l'IA en clarifiant les lois antitrust</strong>    <ul>      <li>Les régulateurs devraient fournir des orientations sur la coordination permise en matière de sécurité de l'IA dans le cadre des lois antitrust actuelles. Clarifier comment les entreprises privées peuvent travailler ensemble dans l'intérêt public sans enfreindre les lois antitrust permettrait de réduire l'incertitude juridique et de promouvoir des objectifs communs.</li>    </ul>  </li></ul><p>Nous pensons que cet ensemble de recommandations nous rapprochera de manière significative de l'établissement d'un cadre efficace pour la responsabilité de l'IA. Cela nécessitera la collaboration entre les chercheurs, les laboratoires d'IA, les régulateurs, les auditeurs et d'autres parties prenantes. Anthropic s'engage à soutenir les efforts visant à permettre le développement et le déploiement sûrs des systèmes d'IA. Les évaluations, les tests de validation externe, les normes, la recherche sur l'interprétabilité, l'audit et les bonnes pratiques en matière de cybersécurité sont autant de pistes prometteuses pour atténuer les risques de l'IA tout en en tirant parti.</p><p>Nous pensons que l'IA pourrait avoir des effets transformateurs de notre vivant et nous voulons nous assurer que ces effets sont positifs. La création de mécanismes solides de responsabilité et d'audit de l'IA sera essentielle pour atteindre cet objectif. Nous sommes reconnaissants d'avoir eu l'occasion de répondre à cette demande de commentaires.</p><p>Vous pouvez lire notre soumission complète <a href="https://cdn2.assets-servd.host/anthropic-website/production/images/Anthropic-NTIA-Comment.pdf">ici</a>.</p>