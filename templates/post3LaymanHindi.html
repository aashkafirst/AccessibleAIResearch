<h1>संवैधानिक एआई: एआई प्रतिपुष्टि से हानिहार्मीता</h1><h2>लेमन हिंदी में</h2><h4>संक्षेप</h4><p>  जब तक AI सिस्टम और ताकतवर होते जाते हैं, हमें इनकी सहायता से अन्य AI की निगरानी करनी चाहिए। हम अपरिहार्य नुकसानदायक आउटपुटों की पहचान करने के लिए किसी भी मानवीय लेबल के बिना एक हानिकारक AI सहायक को प्रशिक्षित करने के तरीकों का प्रयोग करते हैं। एक सूची नियमों या सिद्धांतों के माध्यम से मानवीय पारदर्शिता प्रदान की जाती है, इसलिए हम इस तकनीक को 'संवैधानिक AI' के रूप में संदर्भित करते हैं। इस प्रक्रिया में संपर्क में संवर्धित अवस्था और पुनर्संशोधन शामिल होते हैं। संपर्क में हम प्राथमिक मॉडल से नमूना लेते हैं, फिर स्वयं की आलोचना और संशोधन उत्पन्न करते हैं, और फिर संशोधित प्रतिक्रियाओं पर मूल मॉडल को संदर्भित करते हैं। RL में, हम संदर्भित मॉडल से नमूना लेते हैं, दो सैंपल्स में से कौन सा बेहतर है उसे मूल्यांकन करने के लिए एक मॉडल का उपयोग करते हैं, और फिर इस AI प्राथमिकताओं के डेटासेट से एक प्राथमिकता मॉडल को प्रशिक्षित करते हैं। इसके बाद, हम प्राथमिकता मॉडल का उपयोग करके RL के साथ प्रशिक्षण करते हैं, अर्थात् 'AI प्रतिक्रिया से RL' (RLAIF) का उपयोग करते हैं। इस परिणामस्वरूप, हम हानिकारक पर असर न करने वाले AI सहायक को प्रशिक्षित कर सकते हैं, जो हानिकारक सवालों के साथ संवाद स्थापित करके अपने आप के आपत्तियों की व्याख्या करता है। SL और RL दोनों तरीके मनुष्यों द्वारा मान्यता और तथ्यपूर्णता की दृष्टि से AI निर्णय लेने का सुसंगत तरीका बना सकते हैं। ये तरीके मानवीय लेबल की बहुत कम संख्या के साथ AI के व्यवहार को सटीकता से नियंत्रित करने की संभावना कराते हैं।</p><h4>नीति सूचना पत्र</h4><p>  अधिक विस्तृत जानकारी के लिए 'संवैधानिक AI नीति सूचना पत्र' का अध्ययन करने के लिए यहाँ जाएँ: <a href="https://cdn2.assets-servd.host/anthropic-website/production/images/Anthropic_ConstitutionalAI_v2.pdf">संवैधानिक AI नीति सूचना पत्र</a>।</p>