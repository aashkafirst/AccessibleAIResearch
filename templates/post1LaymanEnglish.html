<h1> Core Views on AI Safety: When, Why, What, and How </h1><h2>In Layman English</h2><p><br></p><p>We started Anthropic because we think AI could change the world a lot, like how science and factories did. But we're worried this could happen soon and not go well. </p><p>This might sound strange, and there are good reasons to not believe it. Almost everyone who says "what we're working on could be really important!" ends up being wrong. But we think there's enough proof to get ready for AI that improves fast and changes everything.</p> <p>At Anthropic, we like to show instead of tell. We've put out a lot of research on how to make AI safe, which we think helps everyone working on AI. We're writing this now because more people know AI is getting better fast. We want to explain what we think and what we're doing. To put it simply, we believe AI safety research is urgent and important, and should be supported by many groups.</p><p>Here, we'll explain why we think AI will improve very fast and change a lot, and why that made us worry about AI safety. We'll also briefly summarize our approach to AI safety research and why we're taking that approach. We hope this helps discuss AI safety and progress.</p><p>To summarize the main points here:</p><ul><li><strong> AI could have a huge impact, maybe in the next 10 years</strong><br><strong></strong>AI keeps getting better fast because we use more and more computer power to train it. Studies show more computing means AI gets more capable. Simple guesses suggest AI could match or beat humans at most mental tasks in the next 10 years. AI progress might slow down or stop, but signs say it'll probably keep going.</li> <li><strong>We don't know how to train AI to behave well consistently</strong><br><strong></strong>No one knows how to train very advanced AI to be helpful, honest, and safe. Also, fast AI progress will change society a lot and could lead companies or countries to use untrustworthy AI. This could be catastrophic, either because AI chooses dangerous goals or makes innocent but dangerous mistakes when a lot is at stake.</li><li><strong>We're most hopeful about a mix of ways to study AI safety</strong><br><em><strong></strong></em>We're exploring ways to build reliably safe AI. We're currently most excited about closely watching AI as it trains, understanding how AI works, learning in a structured way, and evaluating how AI learns and applies knowledge. A key goal is to speed up safety work and consider many scenarios, from ones where safety is easy to achieve to ones where it's extremely hard.</li> </ul><h3>Our Rough View on Rapid AI Progress</h3><p> The three main things that make AI systems smarter over time are <a href="#footnote-1"><sup>1</sup></a>  training data, computation power,  and better algorithms. In the mid-2010s, some scientists noticed that bigger AI systems were usually smarter. So, they thought the most important thing for AI's progress might be how much computation is used to train the systems. When they drew this on a graph, it was clear that computation going into the biggest AI models was growing at <a href="https://openai.com/blog/ai-and-compute/"><span>10 times</span></a> each year - much faster than Moore's Law. In 2019, some of these scientists precisely showed that you could make AI systems smarter in a predictable way just by making them bigger and training them on more data. Using these results, this team led the work to train <a href="https://arxiv.org/abs/2005.14165"><span>GPT-3</span></a>, maybe the first big language model <sup>2</sup>.</p> <p>Since finding out how to scale up AI, many at Anthropic think AI progress could be very fast. But, in 2019, it seemed like needing many types of data, logical thinking, learning speed, using knowledge from one task for another, and long-term memory might slow or stop AI progress. Since then, some of these "walls" like needing many types of data and logical thinking have come down. So, most at Anthropic now think AI progress will keep going instead of slowing down or stopping. AI systems are now almost as good as humans at many tasks, but training them still costs way less than big science projects like the Hubble Space Telescope. So, there's lots of room to keep improving them.<sup>3</sup></p> <p>People usually don't notice or accept exponential growth early on. Although we're seeing fast AI progress, many think this must be temporary and that things will go back to normal soon. But, if we're right, the feeling of fast AI progress may not end before AI systems have a wide range of skills that are better than ours. Also, using advanced AI to make AI progress could speed this up even more; we already see this happening with AI that helps researchers be more productive, and <a href="https://arxiv.org/abs/2212.08073"><span>Constitutional AI</span></a> reducing how much we depend on people.   </p><p>If any of this is right, AI may be able to do most knowledge work in the not too distant future. This would have a huge effect on society and likely speed up progress in other technologies too (like how AlphaFold is already helping biology). What future AI systems will be like - whether they'll act on their own or just give information to humans - is still unknown. Still, this could be a pivotal time. While slower AI progress might be easier to deal with, we have to get ready for what we think will happen, not what we hope for.</p> <p>Of course, all of this could be totally wrong. At Anthropic, we think it's more likely than not, but maybe we're biased because we work on AI. Even if we're wrong though, this possibility is worrying enough that AI companies, lawmakers, and society should put serious work into researching and planning how to handle AI that could greatly change the world.<br/></p><h3>What Safety Risks?</h3><p>If you believe what I said before, AI could be dangerous. There are two simple reasons why we should worry. </p> <p>First, it may be hard to make safe, reliable and controllable AI systems once they get as smart as the people who built them. Using an example, it's easy for a chess expert to see bad moves from a beginner but hard for the beginner to see bad moves from an expert. If we make AI smarter than us but wanting different things, it could end badly. This is the problem of making AI's goals match ours.</p> <p>Second, fast AI progress would change a lot, like jobs, the economy and power between countries. These big changes could be disasters on their own, and also make it harder to develop AI carefully. This could lead to more problems.  </p>  <p>We think if AI progresses fast, these risks will be huge. They could also make each other worse in ways we can't predict. We may later decide we were wrong and they won't be issues or will be easy to fix. But we must be cautious  because being wrong could be terrible.</p> <p>We've already seen AI behave in ways creators didn't want, like being toxic, biased, unreliable, dishonest, and recently, <a href="https://arxiv.org/pdf/2212.09251.pdf" target="_blank"><span>eager to please and wanting power</span></a>. As AI spreads and gets more powerful, these problems will matter more. Some may show the issues we'll have with human-level AI and beyond.</p>  <p>In AI safety, we expect a mix of <a href="https://dl.acm.org/doi/pdf/10.1145/3531146.3533229" target="_blank"> <span> predictable and surprising changes</span></a>. Even fixing issues with today's AI, we can't assume we'll solve future problems the same way. Scary, speculative issues may only appear once AI understands its place, can trick people well or has strategies we don't get. Many worrying problems may arise only when AI is very advanced.</p><h3>Our Approach: Empiricism in AI safety</h3><p>We think it's hard to make fast progress in science without closely watching what we're studying. Repeating tests against the "truth" usually helps science move forward. For AI safety, our main truth comes from experiments training and testing AI. </p><p> This doesn't mean theory isn't important for AI safety. But research based on what we see will likely matter most and have the biggest impact. The types of AI, safety issues and solutions are huge and hard to figure out just by thinking. It's easy to focus on problems that won't happen or miss big ones that will. Solid experiments often make theory and ideas better. </p> <p>The ways to find and fix safety problems will probably be extremely hard to plan ahead of time. They'll need step-by-step work. So, while we may have short-term plans, we're ready to change them as we learn.  We can't promise our current research will succeed, but that's true of any research. </p><h3>The Role of Frontier Models in Empirical Safety</h3><p>Anthropic exists mainly because we think it's important to research safety for advanced AI. This needs a group that can work with big models and focus on safety.</p> <p>Testing with what we see doesn't always mean needing work on cutting-edge systems. You could imagine  safely studying smaller, less powerful models. But that's not our situation. Huge models are very different from smaller ones, including sudden, unexpected changes. Size also connects directly to safety:</p><ul><li>Our biggest worries may only come up with human-level AI, and we'd need that AI to make progress.  </li><li> Safety methods like Constitutional AI or Debate can only work on large models. Smaller models make exploring and proving them impossible. </li><li>We focus on future models' safety, so we must see how methods and properties change as models scale. </li> <li>If future huge models are very dangerous, we'd need them to show clear evidence. </li> </ul> <p>Unfortunately, if we need big models for solid safety research, it forces a hard choice. We must avoid speeding up unsafe technology. But too much caution could slow vital research by only using models far behind the frontier. Also, research isn't enough - we need to build knowledge to quickly apply the latest safety research to real systems.</p><p>Making these choices responsibly requires balance. These concerns drive all we do, including research, management, hiring, deploying systems, security and partnerships. Soon, we plan to commit to only developing advanced models if we meet safety standards. And we'll let an outside group evaluate our models' abilities and safety. </p><h3>Taking a Portfolio Approach to AI Safety</h3><p>Some researchers care about safety because of strong views on AI risks. But even predicting how AI may act soon is hard. Guessing how safe future systems will be seems harder still. Rather than take a firm stance, we think many outcomes are possible.</p> <p>An important unknown is how hard it will be to make advanced AI broadly safe with little human risk. It could range from very easy to impossible. Let's call them:</p><ol><li><strong>Optimistic:</strong> Little chance of disaster from advanced AI safety issues. Methods like <a href="https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf" target="_blank">reinforcement learning from human feedback</a> (RLHF) and <a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional AI</a> (CAI) likely enough. Main risks are today's issues (toxicity, misuse) and automation/global impacts - needing research. </li> <li><strong>Intermediate:</strong> Disaster is possible or even likely. But focused work may achieve safety.</li><li><strong>Pessimistic:</strong> AI safety is unsolvable. We can't align a system smarter than us, so don't develop advanced AI.</li></ol><p><strong> If optimistic...</strong> What Anthropic does matters less; disaster unlikely either way. We'll speed beneficial advanced AI and reduce near-term harms. We may help address <a href="https://www.governance.ai/research-paper/thinking-about-risks-from-ai-accidents-misuse-and-structure" target="_blank">structural risks</a>, the biggest issue without catastrophic failures.</p>  <p><strong>If intermediate...</strong> Anthropic will find risks from advanced AI and ways to safely train powerful systems. Our methods like Constitutional AI may help in "medium-easy" or "medium-hard" scenarios.  </p><p><strong>If pessimistic...</strong> Anthropic will show advanced AI safety can't prevent disaster, raising the alarm so we avoid dangerous AI. Near-pessimistic: we focus research and halt progress. Signs could come fast and be hard to see. Always assume this scenario unless evidence otherwise.   </p> <p>The stakes mean a top goal is learning our scenario. Our research aims to understand AI and detect issues like <a href="https://arxiv.org/pdf/2206.13353.pdf" target="_blank">power-seeking</a> or deception.</p> <p>We aim for:</p> <ol><li>Better ways to make AI safer </li><li>Better ways to tell how safe/unsafe AI is</li></ol><p>If optimistic, (i) helps beneficial AI; (ii) shows it's safe. If intermediate, (i) may avoid disaster; (ii) ensures low risk. If pessimistic, failing (i) shows AI safety impossible; (ii) proves this to others.</p> <p>We take a "portfolio approach" to AI safety. Rather than bet on one scenario, our research significantly helps intermediate ones, raises alarms in pessimistic ones, and benefits optimistic ones.  </p><h3>The Three Types of AI Research at Anthropic<br/></h3><p>We group our work at Anthropic into three types:</p><ul><li><strong>Making AI Better:</strong> Research to improve AI, like making language models smarter or reinforcement learning algorithms better. We do this work privately since we don't want AI progressing too fast. We made our first big model, Claude, in 2022 and now use it to study safety.</li> <li><strong>Aligning AI:</strong> Work on new ways to train AI to be helpful, honest, and harmless. Things like debate, red-teaming, Constitutional AI, and RLHF. This work doesn't have to be useful now but may be later as AI gets smarter.</li><li><strong>Understanding AI:</strong> Looking at if AI is really aligned, how well our alignment methods work, and if they'll still work as AI gets smarter. This includes interpretability, evaluating language models, red-teaming, and studying how models generalize.</li> </ul><p>You can see "Aligning AI" as an "blue team" and "Understanding AI" as a "red team." The "blue team" develops new methods while the "red team" finds their weaknesses.</p><p>Some argue RLHF wasn't really safety work. We disagree. Useful "Aligning AI" work lays the foundation for techniques on smarter models, like Constitutional AI and AI evaluations.  It also lets AI help with safety research by making models more honest and open to feedback.</p> <p>If AI safety is easy, our "Aligning AI" work may help most. If it's hard, we'll rely more on  "Understanding AI" to find holes in our methods. And if it's nearly impossible, we desperately need "Understanding AI" to show we must stop advanced AI.</p><h3>Our Current Safety Research</h3><p>We're working on many ways to train safe AI. Some main ideas are:</p><ul><li><strong>Making AI understandable:</strong> We want to understand why AI models behave the way they do and if they'll stay aligned as they get smarter.</li> <li><strong>Managing AI:</strong> We need ways to oversee AI that scale as the systems get more advanced.</li><li><strong>New learning methods:</strong> Trying different ways for machines to learn, like focusing more on the learning <i>process</i> itself.</li><li><strong>How AI generalizes:</strong> Understanding how AI can apply knowledge from one task to a new task. This helps ensure the knowledge is applied safely.</li> <li><strong>Finding dangers:</strong> We test AI systems to check for harmful behaviors we want to avoid as they become more capable.</li><li><strong>Impacts on society:</strong> We study how AI may influence the world to guide progress in a good direction.</li> </ul><h4>Making AI understandable</h4><p>Solving the problem of unsafe AI relies on detecting undesirable behaviors. If we can find these even in new situations by "reading AI's mind," we have a better chance of training safe AI. For now, we can warn others that an AI system is unsafe.</p><p>Our interpretability work focuses on gaps left by other safety research. The most valuable thing would be recognizing if an AI is faking alignment on hard tests. If our work on managing and new learning methods succeeds, models may seem aligned but actually be optimistic or pessimistic. Interpretability may be the only way to tell.</p><p>This leads us to a risky goal: reverse engineering neural networks into human-understandable algorithms, like auditing code. Language models are complex programs, and "superposition" makes it harder, but we see signs this can work. Before Anthropic, some of us found vision models have interpretable circuits. We've extended this to small language models and found a mechanism for in-context learning. We also better understand neural computation, like memorization.</p> <p>This is our current direction. We're guided by evidence, so will change direction if needed. Understanding neural networks and learning in detail gives more ways to pursue safety.</p><p>If interpretability works, the future is optimistic or intermediate. If not, it's pessimistic. We aim for safety through advanced techniques, but if they don't work, we build a strong case to halt progress.</p><h4>Managing AI </h4><p>Teaching language models to become helpful AI needs a lot of high-quality feedback. A worry is that <a href="https://distill.pub/2019/safety-needs-social-scientists" rel="noreferrer noopener" style="text-decoration:none;" target="_blank"><span style="font-size:11pt;font-family:Arial;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;vertical-align:baseline;white-space:pre-wrap;">people can't give enough</span></a>. Maybe people can't give good enough feedback for the AI to learn safety in many situations. Maybe people can be fooled by the AI and give the wrong feedback by accident. Maybe people could give the right feedback but not at a large scale.</p> <p> The only way to get enough good feedback may be for AI to oversee itself or help people oversee it. We need to turn a little high-quality human feedback into a lot of high-quality AI feedback. Methods like RLHF and Constitutional AI show promise, but more work is needed for human-level AI. Language models already learn about human values during pre-training. Bigger models may understand values better and learn them more easily. The goal is for models to understand and follow human values.</p><p>Methods like CAI also allow "red teaming" - automatically generating problems for AI and training it to respond safely. We hope scalable oversight can train safer systems.  We're studying ways to scale oversight, like improving CAI, helping people supervise AI, AI debating itself, "red teaming" with multi-agent RL, and model-generated evaluations. Scaling supervision may be key to exceeding human abilities safely, but more work is needed. </p><h4>New learning methods </h4><p> One way to learn something new is by trying different ways until you get it right. This is called "outcome-oriented learning". In this, how you learn depends only on the goal. You try different strategies until you find an easy way to achieve the goal.   </p><p>A better way to learn is by having an expert teach you the steps they take to succeed. During practice, succeeding isn't as important as improving how you do things. As you get better, you can work with your coach to find even better strategies. This is called "process-oriented learning". The goal here is to master the steps, not just achieve the outcome. </p> <p>Process-oriented learning may help address concerns about advanced AI safety. In this method: </p><ul><li> Humans will understand the steps AI systems take because the steps have to be explained to people. </li><li> AI systems won't be rewarded for achieving goals in confusing or bad ways. They'll be rewarded for clear, useful steps. </li><li>AI systems won't be rewarded for problematic goals like getting resources or deceiving others. Humans will give negative feedback for those kinds of steps during training.</li></ul><p>At Anthropic, we like simple solutions. Limiting AI training to process-oriented learning could address many issues with advanced AI. We want to find limits of this method and when outcome-based learning causes safety problems. We think process-oriented learning may be the best way to train safe, transparent systems, even beyond human level. </p><h4> How AI generalizes </h4><p> We're trying to better understand how large language models (LLMs) are trained. </p> <p> LLMs show surprising behaviors, like creativity, self-preservation, and deception. These come from the training data, but the path is complicated. First, models learn from huge amounts of raw text. They develop wide representations and can simulate different agents. Then they're fine-tuned in many ways, some surprising. Fine-tuning depends a lot on what's learned from pretraining using much of the world's knowledge. </p><p>When a model acts like a deceitful but helpful AI, is it just repeating similar training data? Or has that behavior, or the beliefs and values behind it, become how the model thinks AI assistants should act in any situation? We're developing ways to trace a model's outputs to its training data. This can help us understand the model.   </p> <p> At Anthropic, we care about interpreting models and their behaviors. We want to know if behaviors come from specific data or the model's general understanding. Understanding a model's implicit biases and representations from pretraining is key. Surprising model outputs could come from fine-tuning in unintended ways. Our techniques to analyze models and connect their outputs to training data will provide important clues to interpreting them. </p><h4> Finding dangers </h4> <p> A key worry is that advanced AI could develop harmful behaviors, like deception or strategic planning, that simpler AI don't have. We think the way to spot these before they become threats is to deliberately train small, limited models to have these behaviors so we can study them. </p><p> We're especially interested in how AI acts when it knows it's an AI talking to humans in training. Do AI become deceptive or develop surprising, undesirable goals? Ideally, we'll build detailed models of how these change with size so we can foresee dangerous behaviors emerging. </p>  <p>At the same time, this research itself could be risky. It's unlikely if done on small, limited models, but involves creating the behaviors we worry about. It would be risky on bigger, more capable models. We won't do this research on models that could cause real harm.</p> <p>At Anthropic, we want to ensure advanced AI systems behave helpfully. We'll study potentially dangerous failures in small models, but carefully. Key is if AI changes when "situationally aware" - knowing it's AI training with humans.  </p><p>Goals are: see if undesirable behaviors like deception emerge, especially as models scale; build models to predict this; keep research safe. We'll avoid bigger models and serious risks.  But creating and studying these behaviors, even in small models, requires caution.  </p><p>We aim not for dangerous AI, but to advance safely by anticipating problems.  Testing for and understanding failures will help build inherently trustworthy systems. But we must be vigilant and responsible. Overall we think this approach can reduce risks from advanced AI if we're thoughtful, careful and committed to safety.</p><h4>Impacts on society </h4><p> Carefully evaluating how our work could impact society is key. We build tools and measures to assess AI capabilities, limits, and societal impacts. For example, we studied how predictable or surprising large language models can lead to problems. We showed how surprises could be misused. We also tested ways to find and reduce harm in models of different sizes. Recently, we found current language models can follow instructions to reduce bias and stereotyping.</p> <p>We worry how deploying increasingly powerful AI will impact society in the short, medium and long term. We work on projects to: evaluate and mitigate potential harms from AI; predict how AI could be used; study economic impacts. This work also helps develop responsible AI policies and rules. By rigorously studying AI today, we aim to give policymakers and researchers insights and tools to reduce significant societal risks and ensure AI's benefits are spread evenly. </p> <p>At Anthropic, we see advanced AI's responsible development as crucial. Our approach: analyze AI capabilities and limits; anticipate challenges and opportunities; address issues proactively. Societal implications research must continue as progress accelerates. </p><p>Examples of our work: studying predictability/surprise in language models, how it leads to problems; "red teaming" models to find/fix harms; seeing models can reduce bias when asked. We're very concerned with deploying increasingly powerful AI and impacts, and are tackling risks through evaluations, predictions and impact studies to guide policy and governance. </p> <p>The goal is providing information and tools so policymakers and researchers can minimize serious societal risks and ensure benefits are distributed fairly. By studying implications rigorously now, we aim for AI that is trustworthy, beneficial and aligned with human values as progress marches on. Overall we believe proactive, thoughtful work can help increase the chance of a positive long-term future with advanced AI. <h4>Closing thoughts</h4><p>We think AI could change the world a lot, maybe in 10 years. Computers keep getting more powerful, so new AI will be way more advanced than today's. But we don't fully know how to make sure powerful AI aligns with what humans value so the risk of disaster is small. </p><p>Today's AI is not an immediate worry. But we should do work now to lower risks from advanced AI, if much more powerful AI is made. Creating safe AI may be easy, but we need to plan for harder situations.  </p>  <p>Anthropic takes an evidence-based approach to AI safety. We're working to: understand better how AI learns and applies knowledge in the real world; develop ways to oversee and check AI at scale; build transparent, interpretable AI; train AI to follow safe steps rather than just pursue goals; analyze how AI could fail dangerously and prevent it; evaluate AI's effects on society to guide rules and research. </p><p>By addressing AI safety in many ways, we hope to succeed in different scenarios. We'll adjust as we learn more. Goals are: prepare for advanced systems; minimize disaster risks if progress outpaces safeguards. Though today's AI is no immediate threat, we're tackling challenges from fast progress proactively.   </p>  <p> Examples of our approach: Study how models apply learning to complex environments. Create oversight and review methods for AI. Build transparent, interpretable systems. Focus models on safe steps over just goals. Identify dangerous failures and counters. Assess societal impacts to inform policymakers and researchers. </p><p>Thoughtful, evidence-based work now can help ensure longer-term AI progress is aligned, beneficial and trustworthy. We aim for increasingly powerful but inherently safe and ethical AI. By systematically and responsibly addressing safety, we hope to lower risks and build a positive future with advanced technology. But we know the challenges ahead and believe preparation and continued progress are key. </p><h4>Footnotes</h4><ol><li> Progress in developing new AI training methods (algorithmic progress) is hard to measure but seems exponential and faster than Moore's Law. To estimate overall AI progress, multiply exponential growth in funding, computing power and algorithmic progress. </li><li> Scale laws justified the work, but another goal was pivot to AI that can read/write to easily train/test AI that can consider human values.</li> <li> Guessing AI progress from more computing is imperfect and needs judgment. GPT-3 mostly came from 250x more computing than GPT-2. GPT-3 to 2023 state-of-the-art may need 50x more. In 5 years, 1000x more computing could train the largest models, based on trends. If scale laws hold, progress would far surpass GPT-2 to GPT-3. At Anthropic, we know these systems well - that jump could achieve human-level performance in most tasks. This uses informed intuition, so imperfect, but facts like: (i) computing difference in systems; (ii) performance difference; (iii) scale laws to project future systems; (iv) computing cost/spending trends; together suggest over 10% chance of broadly human-level AI in 10 years. This coarse analysis ignores algorithmic progress and unsure of compute numbers, but most disagreement is intuiting future progress from equivalent compute jumps. </li> <li>For example, many assumed local minima would stop neural networks learning, while aspects like widespread adversarial examples were surprises.  </li><li> Effective safety research on large models needs more than nominal access (e.g. API) - to do interpretability, fine-tuning, reinforcement learning, we must develop AI internally at Anthropic. </li></ol><p>In summary, it's hard to precisely measure or predict progress in advanced AI. But there are reasons to believe significant, human-level capabilities could emerge within 10 years, based on:-Exponential increases in funding, computing power, and progress developing new training techniques-Scale laws suggesting big jumps in capability from more computing, as in GPT-2 to GPT-3-Informed intuition at Anthropic, where we build and study these systems directly-Facts like cost/spending trends in computing and differences enabling GPT-3 point to a good chance of major progress. Still, extrapolating is imperfect. Surprises emerge, like adversarial examples. And safety research needs internal development of models, not just access. Overall we think proactively addressing advanced AI's challenges - systematically, carefully and empirically - can help ensure it remains aligned and beneficial as it rapidly progresses. But uncertainty means we must remain vigilant and continue learning. </p>