<h1>AI जवाबदेही के लिए एक मार्ग चित्रित करना</h1><h2>लेमन हिंदी में</h2><p><br/></p><p>इस हफ्ते, ऐंथ्रोपिक ने नेशनल टेलीकम्यूनिकेशंस और सूचना प्रशासन (NTIA) के AI जवाब के लिए एक प्रतिक्रिया जमा की। आज हम हमारी सिफारिशें साझा करना चाहते हैं क्योंकि ये कुछ ऐंथ्रोपिक के मुख्य AI नीति प्रस्तावों को सम्मिलित करती हैं।</p><p>आज के दिन आधुनिक Artificial Intelligence (AI) सिस्टमों की मजबूत और सम्पूर्ण प्रक्रिया का कोई ठोस और समग्र मानक प्रक्रिया नहीं है, और भविष्य के और अधिक सक्षम सिस्टमों की तो दूर की बात है। हमारी प्रतिवेदन में हमने AI जवाबदेही सुनिश्चित करने के लिए आवश्यक प्रक्रिया और इंफ्रास्ट्रक्चर पर हमारी दृष्टि प्रस्तुत की है। हमारी सिफारिशें NTIA की संयोजक संगठन की पोटेंशियल भूमिका को ध्यान में रखकर बनाई गई है, जो अन्य सरकारी एजेंसियों जैसे नेशनल इंस्टीट्यूट ऑफ स्टैंडर्ड्स और टेक्नोलॉजी (NIST) के साथ मिलकर मानक निर्धारित करता है।</p><p>हमारी सिफारिशों में, हम महान सक्षम और सामान्य उद्देश्यकारी AI मॉडलों के लिए जवाबदेही युक्तियाँ पर ध्यान केंद्रित करते हैं। विशेष रूप से, हम सिफारिश करते हैं:</p><ul>  <li>    <strong>बेहतर मूल्यांकन तैयार करने के लिए अनुसंधान को वित्तपोषित करें</strong>    <ul>      <li>AI मॉडल मूल्यांकन अनुसंधान के लिए वित्तीय निधि को बढ़ाएं। सख्त, मानकीकृत मूल्यांकन विकसित करना कठिन और समय लेने वाला काम है जो महत्वपूर्ण संसाधनों की आवश्यकता होती है। विशेषतः सरकारी एजेंसियों से बढ़ीतर वित्तीय सहायता मानक निर्धारित करने में मदद कर सकती है।</li>      <li>नजदीकी भविष्य में कंपनियों को मूल्यांकन विधियों और परिणामों की प्रकटीकरण करने के लिए बाध्य करें। AI सिस्टम डिप्लॉय करने वाली कंपनियों को अपने मूल्यांकन के संबंध में कुछ खुलासे के आवश्यकताओं को पूरा करने के लिए अधिकारित किया जाना चाहिए, हालांकि अगर ऐसा करने से बौद्धिक संपत्ति (IP) या गोपनीय जानकारी का क्षति हो सकता है तो ये आवश्यकताएं सार्वजनिक नहीं की जानी चाहिए। यह पारदर्शिता अनुसंधानकर्ताओं और नीति निर्माताओं को मदद कर सकती है ताकि वे समझ सकें कि मौजूदा मूल्यांकन में कहाँ कमी हो सकती है।</li>      <li>लंबे समय में उद्योग मूल्यांकन मानक और सर्वोत्तम प्रथाएं विकसित करें। NIST जैसे सरकारी एजेंसियाँ AI मॉडल की क्षमताओं, सीमाओं, और जोखिमों का मूल्यांकन करने के लिए मानक और मानदंड स्थापित करने के लिए काम कर सकती हैं जिनका पालन करेंगी कंपनियाँ।</li>    </ul>  </li>  <li>    <strong>मॉडल क्षमताओं पर आधारित जोखिम-प्रतिक्रियात्मक मूल्यांकन तैयार करें</strong>    <ul>      <li>AI सिस्टम के लिए मानक क्षमता मूल्यांकन विकसित करें। सरकारों को पूर्णत: वित्तपोषित और उन्नत AI से होने वाली महत्वपूर्ण जोखिमों पर धोखाधड़ी और स्वतंत्रता जैसे मुद्दों पर संरचित क्षमता और सुरक्षा मूल्यांकन के विकास में भागीदारी करनी चाहिए। ये मूल्यांकन संयोगी, जोखिम-प्रतिक्रियात्मक विनियमिती के लिए प्रमाण-आधारित आधार प्रदान कर सकते हैं।</li>      <li>सुरक्षा मूल्यांकन के लिए और अधिक अनुसंधान और वित्तीय सहायता के माध्यम से एक जोखिम की सीमा तैयार करें। एक जोखिम की सीमा स्थापित हो जाने के बाद, हम इस सीमा के खिलाफ सभी मॉडलों के मूल्यांकन का आदेश जारी कर सकते हैं।        <ul>          <li>यदि कोई मॉडल इस जोखिम की सीमा से कम हो जाता है, तो मौजूदा सुरक्षा मानक पर्याप्त होने की संभावना है। पालन की जांच करें और डिप्लॉय करें।</li>          <li>यदि कोई मॉडल जोखिम की सीमा से अधिक होता है और सुरक्षा मूल्यांकन और सुरक्षा सुरक्षा उपाय अपर्याप्त होते हैं, तो डिप्लॉयमेंट को रोकें, पारदर्शिता को मजबूत करें, और नियामकों को सूचित करें। डिप्लॉय करने से पहले उचित सुरक्षा सुरक्षा निर्धारित करें।</li>        </ul>      </li>    </ul>  </li>  <li>    <strong>बड़ी AI प्रशिक्षण दौड़ों के लिए पूर्व-पंजीकरण स्थापित करें</strong>    <ul>      <li>AI डेवलपर्स के लिए एक प्रक्रिया स्थापित करें ताकि नियामकों को संभावित जोखिमों की जागरूकता हो। इसमें सही प्राप्तकर्ता, आवश्यक जानकारी, और उपयुक्त साइबर सुरक्षा, गोपनीयता, IP, और प्राइवेसी सुरक्षा का निर्धारण शामिल है।</li>      <li>बड़ी प्रशिक्षण दौड़ों का संचित रजिस्ट्री स्थापित करें जिसमें AI डेवलपर्स अपने गृह देश की राष्ट्रीय सरकार के साथ मॉडल विवरणों को पंजीकृत कर सकेंगे (जैसे मॉडल निर्देशिका, मॉडल प्रकार, कंप्यूट इंफ्रास्ट्रक्चर, निर्देशित प्रशिक्षण समापन तिथि, और सुरक्षा योजनाएं) प्रशिक्षण पूरा होने से पहले। संचित रजिस्ट्री डेटा को सर्वोच्च उपलब्ध मानकों और विनिर्दिष्टियों के अनुरूप सुरक्षित किया जाना चाहिए।</li>    </ul>  </li>  <li>    <strong>तीसरे पक्ष निरीक्षकों को सशक्त बनाएं जो...</strong>    <ul>      <li><strong>तकनीकी रूप से सक्षम</strong> - कम से कम कुछ निरीक्षकों को गहरे मशीन लर्निंग अनुभव की आवश्यकता होगी।</li>      <li><strong>सुरक्षा-संवेदी</strong> - मूल्यवान इंटेलेक्चुअल प्रॉपर्टी की सुरक्षा करने के लिए अच्छी तरह स्थानांतरित किए गए हों, जो चोरी हो जाने पर राष्ट्रीय सुरक्षा की धमकी प्रस्तुत कर सकती है।</li>      <li><strong>लचीला</strong> - सुरक्षा की सुरक्षा सुनिश्चित करते हुए मजबूत लेकिन हल्का टिप्पणीय मूल्यांकन कार्य कर सकें। इससे संघर्षों को कैद न करेंगे और संयुक्त राज्यों को नुकसान न पहुंचाएँगे।</li>    </ul>  </li>  <li>    <strong>मॉडल विमोचन से पहले बाहरी लाल टीमिंग का अनिवार्यता निर्धारित करें</strong>    <ul>      <li>AI सिस्टम के लिए बाहरी लाल टीमिंग को अनिवार्यता के रूप में अग्रसर करें, चाहे वो सेंट्रलाइज्ड थर्ड पार्टी (जैसे NIST) के माध्यम से हो या डीसेंट्रलाइज्ड तरीके से (जैसे रिसर्चर एपीआई एक्सेस के माध्यम से) ताकि AI सिस्टमों के विपरीत परीक्षण को मानकीकृत किया जा सके। यह विकसित करने वाले डेवलपर्स के लिए एक पूर्व-शर्त होनी चाहिए जो प्रगतिशील AI सिस्टम जारी कर रहे हैं।</li>      <li>मॉडल विमोचन से पहले उच्च गुणवत्ता वाले बाहरी लाल टीमिंग विकल्प स्थापित करें। यह महत्वपूर्ण है क्योंकि वर्तमान में लाल टीमिंग की क्षमता केवल निजी AI लैब में ही होती है।</li>    </ul>  </li>  <li>    <strong>समझने के लिए अनुवाद अनुसंधान को आगे बढ़ाएं</strong>    <ul>      <li>अनुवाद अनुसंधान के लिए वित्त प्रदान करें। सरकारी अनुदान और प्रोत्साहन विश्वविद्यालयों, गैर-लाभकारी संगठनों, और कंपनियों में समझने के कार्य के लिए प्रोत्साहन प्रदान करें। इससे छोटे मॉडल पर मानदंडी शोध करने की संभावना होगी, जो सीमान्य लैब के बाहर प्रगति को संभव करेगी।</li>      <li>मानदंडों द्वारा अनुवादनीय मॉडल की मांग वर्तमान में संभव नहीं है, लेकिन भविष्य में अनुसंधान की प्रगति के साथ यह संभव हो सकता है।</li>    </ul>  </li>  <li>    <strong>एंटीट्रस्ट के आस-पास AI सुरक्षा पर उद्योग सहयोग को संभव बनाएं</strong>    <ul>      <li>नियामकों को मौजूदा एंटीट्रस्ट कानूनों के अनुसार AI उद्योग सुरक्षा समन्वय पर मार्गदर्शन जारी करना चाहिए। यह स्पष्ट करना की निजी कंपनियाँ एंटीट्रस्ट कानूनों का उल्लंघन किए बिना साझा हित में कैसे काम कर सकती हैं, कानूनी अनिश्चितता को कम करेगा और साझा लक्ष्यों को आगे बढ़ाएगा।</li>    </ul>  </li></ul><p>हम यह मानते हैं कि ये सिफारिशें हमें AI जवाबदेही के लिए एक प्रभावी ढांचा स्थापित करने के करीब ले जाएंगी। इसके लिए शोधकर्ताओं, AI लैब, नियामकों, निरीक्षकों, और अन्य स्तों, के बीच सहयोग की आवश्यकता होगी। ऐंथ्रोपिक सुरक्षा से जुड़े प्रयासों का समर्थन करने के लिए समर्पित है। मूल्यांकन, लाल टीमिंग, मानक, अनुवादनीयता और अन्य सुरक्षा अनुसंधान, निरीक्षण, और मजबूत साइबर सुरक्षा प्रथाएं सभी AI के जोखिमों को कम करने और इसके लाभों को प्राप्त करने के लिए उम्मीदवार हैं।</p><p>हम मानते हैं कि AI हमारे जीवन में परिवर्तनकारी प्रभाव डाल सकती है और हम चाहते हैं कि ये प्रभाव सकारात्मक हों। सुरक्षा और निरीक्षण योजनाओं के मजबूत निर्माण और मानकों की स्थापना को प्राप्त करने के लिए AI जवाबदेही और निरीक्षण तंत्र के सृजन होना महत्वपूर्ण होगा। हम इस सलाह के मौके का आभारी हैं।</p><p>आप हमारी पूरी सबमिशन को <a href="https://cdn2.assets-servd.host/anthropic-website/production/images/Anthropic-NTIA-Comment.pdf">यहाँ</a> पढ़ सकते हैं।</p>