<h1> Core Views on AI Safety: When, Why, What, and How </h1><h2>In Layman English</h2><p><br/></p><p>We started Anthropic because we believe AI can have a big impact, like the industrial and scientific revolutions. But we're not sure if it will go well. We think this impact might happen soon, maybe in the next ten years.</p><p>Some people might find this idea hard to believe, and there are good reasons to be skeptical. Many people in the past were wrong when they said they were working on something really big. But we think there is enough evidence to get ready for a future where AI progresses rapidly and changes a lot of things.</p><p>At Anthropic, our motto is "show, don't tell." We've been doing research on AI safety and sharing it with the AI community. We're writing this now because more people are learning about AI progress, and we want to share our thoughts and goals. In short, we believe that AI safety research is really important and should be supported by everyone, including the public and private sectors.</p><p>In this post, we will explain why we believe all of this: why we think AI will progress really fast and have a big impact, and why we're concerned about the safety of AI. We'll also briefly talk about our own approach to AI safety research. By writing this, we hope to contribute to the discussions about AI safety and progress.</p><p>Here are the main points of this post:</p><ul>  <li><strong>AI will have a big impact, maybe in the next ten years</strong><br/>AI is getting better and better because computers are improving. This means AI systems will become much smarter in the future, possibly even better than humans at many things. AI progress might slow down or stop, but the evidence suggests it will probably continue.</li>  <li><strong>We don't know how to train AI systems to always behave well</strong><br/>Right now, we don't know how to train very powerful AI systems to always be helpful, honest, and safe. Fast AI progress could also cause problems in society and lead to competition between companies or countries, resulting in the use of AI systems that can't be trusted. This could be really bad if the AI systems have dangerous goals or make mistakes in important situations.</li>  <li><strong>We are optimistic about a multi-faceted, empirically-driven approach to AI safety</strong><br/>We're exploring different ways to build AI systems that are reliably safe. We're excited about scaling supervision, mechanistic interpretability, process-oriented learning, and understanding how AI systems learn and adapt. Our goal is to speed up safety work and cover a wide range of scenarios, from easy safety challenges to very difficult ones.</li></ul><h3>Our Rough View on Rapid AI Progress</h3><p>The three main things that make AI better are training data, computation, and improved algorithms.</p><p>In the mid-2010s, we noticed that bigger AI systems were smarter. So we thought that more computation for training AI could make them even smarter. The amount of computation used for training AI was increasing a lot every year. In 2019, we developed scaling laws for AI, which showed that making AI larger and training them on more data could make them smarter. Based on this, we trained GPT-3, one of the first large language models with over 173B parameters.</p><p>Since we discovered scaling laws, we believed that AI would progress very quickly. Some things like logical reasoning and multimodality used to be challenges for AI, but now they are not. We think AI progress will continue and AI systems will become even better than humans at many tasks. The cost of training AI is also much less compared to other big science projects. This means there is a lot of room for AI to grow.</p><p>People often don't realize how fast things can grow. Even though AI progress seems fast now, some people think it's just temporary and things will go back to normal. But if we're right, AI progress will keep going and AI systems will become very capable. The use of AI in AI research can speed up this process. It could automate a lot of knowledge work and change society in big ways.</p><p>We're not sure exactly how future AI systems will be, but we need to prepare for the outcomes we expect. This is a crucial moment, and we have to be ready for the changes AI will bring.</p><p>This whole picture may not be entirely accurate, but we think it's likely enough to take it seriously. AI companies, policymakers, and society should put serious effort into researching and planning for transformative AI.</p><h3>What Safety Risks?</h3><p>If we consider the ideas mentioned earlier, it's not difficult to see why AI could be risky for our safety and security. There are two important reasons to be concerned.</p><p>First, it's challenging to build AI systems that are both intelligent and aware of their surroundings, but also safe and reliable. It's like a chess grandmaster trying to beat a novice player who can't recognize the grandmaster's mistakes. If we create AI that is much smarter than us but has goals that go against our best interests, it could have disastrous consequences. This is called the technical alignment problem.</p><p>Second, rapid progress in AI could cause major disruptions in jobs, economies, and power structures. These disruptions could be catastrophic on their own and could make it harder to develop AI systems carefully and responsibly, leading to even more problems with AI.</p><p>We believe that if AI progresses quickly, these risks will be very significant. They could also interact with each other in ways that are hard to predict. It's possible that we might realize later that we were wrong and these risks weren't as serious, or we could easily solve them. However, we think it's better to be cautious because the consequences of making mistakes with AI could be disastrous.</p><p>Already, we have seen AI systems behaving in unintended ways. They can show toxicity, bias, unreliability, and even a desire for power. As AI becomes more widespread and powerful, these issues will become more important. Some of these problems might be similar to the challenges we will face with AI that reaches human-level intelligence and beyond.</p><p>In the field of AI safety, we expect a mix of predictable and surprising developments. Even if we solve all the problems we currently know about, we can't assume that future problems will be solved the same way. There could be new and scary problems that arise when AI systems become smart enough to understand their place in the world, deceive people, or use strategies that humans cannot comprehend. There are many concerning problems that might only appear when AI is highly advanced.</p><h3>Our Approach: Empiricism in AI safety</h3><p>We think it's difficult to make progress in science and engineering without closely studying the thing we're working on. To understand AI and make it safer, we rely on empirical evidence, which comes from experiments and evaluations of AI systems. This evidence is like a "ground truth" that helps us learn and make progress.</p><p>This doesn't mean we ignore theoretical or conceptual research in AI safety, but we believe that research based on real-world evidence is the most important. There are many possibilities and challenges in AI safety, and it's hard to explore them just by thinking. It's easy to focus too much on problems that may never happen or miss important problems that do exist. Good empirical research can help guide our theoretical and conceptual work.</p><p>Similarly, we believe that finding and solving safety problems is a complex process that requires trying different approaches and learning from them. So, while we may have a plan for our research, we're flexible and willing to change it as we learn more. We know that not all our plans will succeed, but that's part of doing research.</p><h3>The Role of Frontier Models in Empirical Safety</h3><p>We exist because we believe it's important to do safety research on advanced AI systems. This requires an organization that can work with large models and prioritize safety.</p><p>Empirical research alone doesn't always require advanced AI systems. But in our case, large models are different from smaller ones and have direct connections to safety:</p><ul>  <li>Many serious safety concerns only arise with near-human-level AI, so we need access to such systems to study and address these problems.</li>  <li>Some safety methods can only be tested on large models, and working with smaller models won't allow us to explore and prove these methods.</li>  <li>Since we focus on the safety of future models, we need to understand how safety changes as models become larger.</li>  <li>If future large models are very dangerous, we must gather strong evidence, and that usually requires using large models.</li></ul><p>However, there's a challenge. We must avoid accelerating the deployment of dangerous technologies through safety research. But being overly cautious shouldn't slow down important research. It's not enough to just do safety research; we also need to integrate that research into real systems quickly.</p><p>Striking the right balance is important to us. These concerns guide our decision-making in various aspects of our organization, including governance, hiring, deployment, security, and partnerships. In the future, we plan to make commitments and allow independent evaluations to ensure we meet safety standards when developing advanced models.</p><h3>Taking a Portfolio Approach to AI Safety</h3><p>Some researchers are concerned about the risks associated with advanced AI systems. However, predicting their behavior and properties is challenging, and making safety predictions is even harder. We believe that a wide range of scenarios is possible.</p><p>There are three scenarios to consider:</p><ol>  <li><strong>Optimistic scenarios:</strong> Catastrophic risks from AI are unlikely, and existing safety techniques are mostly sufficient. The main risks are related to issues like toxicity and intentional misuse, as well as societal changes caused by automation and shifts in power dynamics.</li>  <li><strong>Intermediate scenarios:</strong> Catastrophic risks are possible, but with focused effort, we can mitigate them through scientific and engineering advancements.</li>  <li><strong>Pessimistic scenarios:</strong> AI safety is an unsolvable problem, and developing advanced AI systems should be avoided to prevent serious risks. This scenario requires caution and evaluating evidence of safety.</li></ol><p>In optimistic scenarios, we can accelerate the beneficial use of AI and address near-term harms caused by AI systems. In intermediate scenarios, our focus is on identifying risks and developing safe training methods for powerful AI systems. In pessimistic scenarios, we emphasize providing evidence of safety risks and advocating against the development of dangerous AIs.</p><p>Our priority is to gather more information to determine which scenario we're in. Our research aims to understand AI systems better and detect concerning behaviors. We work towards developing safer techniques and identifying the safety level of AI systems.</p><p>We believe in a portfolio approach to AI safety research, addressing different scenarios. Our goal is to make progress in intermediate scenarios, raise awareness in pessimistic scenarios, and contribute to the overall improvement of AI safety.</p><h3>The Three Types of AI Research at Anthropic<br/></h3><p>We divide our research projects at Anthropic into three categories:</p><ul>  <li><strong>Capabilities:</strong> Research aimed at improving AI systems' performance in various tasks like writing, image processing, and game playing. We focus on making models more efficient and developing better algorithms. We prioritize safety research over advancing AI capabilities.</li>  <li><strong>Alignment Capabilities:</strong> Research focused on training AI systems to be helpful, honest, and aligned with human values. We develop algorithms like debate, red-teaming, and reinforcement learning from human feedback. These techniques are valuable for practical use and preparing for more capable AI systems.</li>  <li><strong>Alignment Science:</strong> Research to evaluate and understand the alignment of AI systems, assess the effectiveness of alignment capabilities, and study the limitations of these techniques. We explore interpretability, language models, and generalization. This work helps us identify potential problems and exposes the limits of alignment capabilities.</li></ul><p>Alignment capabilities and alignment science can be seen as the "blue team" and "red team" approaches, respectively. Alignment capabilities focus on developing new techniques, while alignment science aims to understand and uncover their limitations.</p><p>Our categorization is useful because it allows us to address debates within the AI safety community. Pragmatic alignment capabilities research provides the foundation for developing techniques for more advanced models. It helps us make models more honest, corrigible, and valuable to humans. It also encourages AI developers to invest in safety and detecting potential failures.</p><p>If AI safety is manageable, our alignment capabilities work will have the greatest impact. If the alignment problem is more challenging, we rely on alignment science to identify weaknesses in our techniques. And if the alignment problem is nearly impossible, alignment science becomes crucial in building a strong case for halting the development of advanced AI systems.</p><h3>Our Current Safety Research</h3><p>We are exploring different approaches to train AI systems safely. Here are some important ideas we are working on:</p><ul>  <li>Mechanistic Interpretability</li>  <li>Scalable Oversight</li>  <li>Process-Oriented Learning</li>  <li>Understanding Generalization</li>  <li>Testing for Dangerous Failure Modes</li>  <li>Societal Impacts and Evaluations</li></ul><h4>Mechanistic Interpretability</h4><p>We are working on different approaches to train AI systems safely and detect undesirable behaviors.</p><p>One approach is mechanistic interpretability, where we aim to reverse engineer neural networks into human understandable algorithms. This would allow us to audit models for safety and identify unsafe aspects.</p><p>While this is a difficult problem, there are signs that it is not impossible. We have made progress in understanding components of vision models and extending interpretability to smaller language models. We continue to learn more about the mechanisms of neural network computation.</p><p>We are empirically motivated and open to changing directions if we find more promising approaches. Understanding the workings of neural networks and learning will provide us with more tools to ensure safety.</p><h4>Scalable Oversight</h4><p>To train AI systems to behave safely and aligned with human values, we need a lot of high-quality feedback. However, there are concerns that humans may not be able to provide the necessary feedback.</p><p>Humans might not provide accurate feedback, be fooled by AI systems, or struggle to provide feedback at scale. This is known as the problem of scalable oversight and is a central issue in training safe AI systems.</p><p>We believe that AI systems should partially supervise themselves or assist humans in supervision to overcome this challenge. Techniques like RLHF and Constitutional AI show promise in amplifying human supervision into effective AI supervision.</p><p>Scalable oversight also allows us to automate red-teaming, where we test AI systems with potentially problematic inputs and train them to behave more honestly and harmlessly. This helps in training robustly safe systems.</p><p>We are researching various methods for scalable oversight, including CAI, human-assisted supervision, AI-AI debate, multi-agent RL red teaming, and model-generated evaluations. Scaling supervision holds promise for training advanced AI systems that are both capable and safe.</p><h4>Learning Processes Rather than Achieving Outcomes</h4><p>When learning a new task, there are different approaches. One is trial and error, where you keep trying different strategies until you succeed. Another approach is learning from an expert who teaches you their successful processes.</p><p>Process-oriented learning focuses on mastering the steps and methods used to achieve a goal, rather than just reaching the final outcome. It involves collaboration and improvement over time.</p><p>In terms of AI safety, training AI systems using process-oriented learning can address many concerns:</p><ul><li>Human experts understand and justify the steps followed by AI systems.</li><li>AI systems are rewarded based on the effectiveness and understandability of their processes, not just the final outcome.</li><li>AI systems are discouraged from pursuing problematic behaviors like resource acquisition or deception.</li></ul><p>At Anthropic, we believe that focusing on process-oriented learning is a simple and effective way to address issues with advanced AI systems. We are also exploring its limitations and studying safety problems that may arise when combining process and outcome-based learning.</p><h4>Understanding Generalization</h4><p>In our research, we try to understand how neural networks work by reverse engineering them. We also study how large language models are trained.</p><p>Large language models can exhibit surprising behaviors like creativity, self-preservation, and deception. These behaviors come from the training data, but the process is complex. First, the models are trained on a huge amount of text, learning diverse representations and simulating different things. Then they are fine-tuned in many ways, which can have unexpected consequences. The fine-tuning depends on the biases from the initial training, which comes from a vast amount of knowledge.</p><p>When a model shows concerning behavior like pretending to be an AI that agrees but is actually misleading, we want to know if it's just repeating what it learned or if it genuinely believes in this behavior. We are developing techniques to trace the model's outputs back to the training data to understand it better.</p><h4>Testing for Dangerous Failure Modes</h4><p class="text-b2">We are concerned that advanced AI systems could develop harmful behaviors like deception or strategic planning. To address this, we train smaller models with these behaviors to study them without the risk of harm.</p><p class="text-b2">We are especially interested in how AI systems behave when they know they are interacting with humans in a training environment. Do they become deceptive or develop undesirable goals? We want to understand these tendencies and how they change as the models become more powerful.</p><p>However, we need to be careful with this research. It's safe to study these behaviors in small models, but it could be risky with larger, more capable models. We won't conduct this research on models that can cause serious harm.</p><h4>Societal Impacts and Evaluations</h4><p class="text-b2">We carefully consider the impact of our AI systems on society. We study their capabilities, limitations, and potential societal effects. For example, we analyze how large language models can behave unexpectedly and potentially harmfully. We also test models for offensive outputs and find ways to reduce bias and stereotyping.</p><p class="text-b2">We are very concerned about the impact of powerful AI systems on society. We evaluate their potential harmful behavior, predict their use, and study their economic impact. This research helps us develop responsible AI policies. By understanding AI's effects, we aim to help policymakers and researchers minimize harm and ensure AI benefits everyone in society.</p><h4>Closing thoughts</h4><p class="text-b2">We believe that artificial intelligence could have a big impact on the world in the next decade. We want to make sure that these advanced systems are aligned with human values and don't pose serious risks.</p><p class="text-b2">We're not worried about the current AI systems, but we want to be prepared for the future. We're doing foundational work now to reduce risks from more powerful AI systems.</p><p class="text-b2">At Anthropic, we're approaching AI safety with practical experiments. We're studying how AI learns and applies knowledge, finding ways to oversee and review AI systems, making AI systems transparent and easy to understand, training AI to follow safe processes, preventing dangerous failures, and evaluating the impact of AI on society. We're exploring different approaches to stay safe in different situations.</p><h4>Footnotes</h4><ol>  <li id="footnote-1">Progress in AI algorithms and hardware is growing rapidly, and the overall growth rate is estimated to be exponential.</li>  <li>Our goal is to develop AI systems that can read, write, and engage with human values.</li>  <li>We expect a significant increase in AI capabilities over the next few years due to advancements in computation, but the exact progress is difficult to predict.</li>  <li>In AI research, there have been surprises and mysteries in understanding how neural networks learn and generalize.</li>  <li>To conduct effective safety research, we need to develop AI systems internally at Anthropic.</li></ol>