<h1>Points essentiels sur la sécurité de l'IA : Quand, pourquoi, quoi et comment</h1><h2>En termes simples en français</h2><p><br/></p><p>Nous avons créé Anthropic car nous pensons que l'IA peut avoir un impact important, comme les révolutions industrielles et scientifiques. Mais nous ne sommes pas sûrs que tout se passe bien. Nous pensons que cet impact pourrait se produire bientôt, peut-être dans les dix prochaines années.</p><p>Certaines personnes pourraient trouver cette idée difficile à croire, et il y a de bonnes raisons d'être sceptique. De nombreuses personnes dans le passé se sont trompées lorsqu'elles prétendaient travailler sur quelque chose de vraiment grand. Mais nous pensons qu'il y a suffisamment de preuves pour se préparer à un avenir où l'IA progresse rapidement et change beaucoup de choses.</p><p>À Anthropic, notre devise est "montrer, ne pas dire". Nous avons effectué des recherches sur la sécurité de l'IA et les avons partagées avec la communauté de l'IA. Nous écrivons cela maintenant car de plus en plus de personnes apprennent les progrès de l'IA, et nous voulons partager nos réflexions et nos objectifs. En bref, nous pensons que la recherche sur la sécurité de l'IA est vraiment importante et doit être soutenue par tout le monde, y compris le secteur public et privé.</p><p>Dans cet article, nous expliquerons pourquoi nous croyons tout cela : pourquoi nous pensons que l'IA progressera très rapidement et aura un impact majeur, et pourquoi nous nous préoccupons de la sécurité de l'IA. Nous parlerons également brièvement de notre propre approche de la recherche sur la sécurité de l'IA. En écrivant cela, nous espérons contribuer aux discussions sur la sécurité et les progrès de l'IA.</p><p>Voici les points principaux de cet article :</p><ul>  <li><strong>L'IA aura un impact majeur, peut-être dans les dix prochaines années</strong><br/>L'IA s'améliore de plus en plus car les ordinateurs progressent. Cela signifie que les systèmes d'IA deviendront beaucoup plus intelligents à l'avenir, peut-être même meilleurs que les humains dans de nombreux domaines. Les progrès de l'IA pourraient ralentir ou s'arrêter, mais les preuves suggèrent qu'ils vont probablement continuer.</li>  <li><strong>Nous ne savons pas comment entraîner les systèmes d'IA à toujours bien se comporter</strong><br/>Pour le moment, nous ne savons pas comment entraîner des systèmes d'IA très puissants à être toujours utiles, honnêtes et sûrs. Les progrès rapides de l'IA pourraient également causer des problèmes dans la société et conduire à une compétition entre les entreprises ou les pays, ce qui pourrait entraîner l'utilisation de systèmes d'IA qui ne peuvent pas être fiables. Cela pourrait être vraiment mauvais si les systèmes d'IA ont des objectifs dangereux ou commettent des erreurs dans des situations importantes.</li>  <li><strong>Nous sommes optimistes quant à une approche de la sécurité de l'IA multidimensionnelle et basée sur des données empiriques</strong><br/>Nous explorons différentes façons de construire des systèmes d'IA qui soient fiablement sûrs. Nous sommes enthousiasmés par la supervision à grande échelle, l'interprétabilité mécanistique, l'apprentissage orienté vers les processus et la compréhension de la façon dont les systèmes d'IA apprennent et s'adaptent. Notre objectif est d'accélérer les travaux de sécurité et de couvrir un large éventail de scénarios, des défis de sécurité faciles à ceux qui sont très difficiles.</li></ul><h3>Notre point de vue sur les progrès rapides de l'IA</h3><p>Il y a trois choses principales qui rendent l'IA meilleure : les données d'entraînement, le calcul et les algorithmes améliorés.</p><p>À partir du milieu des années 2010, nous avons remarqué que les plus grands systèmes d'IA étaient plus intelligents. Nous avons donc pensé que plus de calcul pour l'entraînement de l'IA pourrait les rendre encore plus intelligents. La quantité de calcul utilisée pour l'entraînement de l'IA augmentait beaucoup chaque année. En 2019, nous avons développé des lois d'échelle pour l'IA, qui ont montré que rendre l'IA plus grande et l'entraîner sur plus de données pouvait la rendre plus intelligente. Sur cette base, nous avons entraîné GPT-3, l'un des premiers grands modèles de langage avec plus de 173 milliards de paramètres.</p><p>Depuis que nous avons découvert les lois d'échelle, nous croyons que l'IA progressera très rapidement. Certaines choses comme le raisonnement logique et la multimodalité étaient autrefois des défis pour l'IA, mais ce n'est plus le cas maintenant. Nous pensons que les progrès de l'IA vont se poursuivre et que les systèmes d'IA seront même meilleurs que les humains dans de nombreuses tâches. Le coût de l'entraînement de l'IA est également beaucoup moins élevé par rapport à d'autres grands projets scientifiques. Cela signifie qu'il y a beaucoup de potentiel de croissance pour l'IA.</p><p>Les gens ne se rendent souvent pas compte à quelle vitesse les choses peuvent croître. Même si les progrès de l'IA semblent rapides maintenant, certaines personnes pensent que c'est juste temporaire et que les choses reviendront à la normale. Mais si nous avons raison, les progrès de l'IA vont se poursuivre et les systèmes d'IA deviendront très performants. L'utilisation de l'IA dans la recherche en IA peut accélérer ce processus. Cela pourrait automatiser beaucoup de travaux de connaissances et changer la société de manière importante.</p><p>Nous ne sommes pas sûrs exactement à quoi ressembleront les futurs systèmes d'IA, mais nous devons nous préparer aux résultats que nous prévoyons. C'est un moment crucial, et nous devons être prêts pour les changements que l'IA apportera.</p><p>Cette image globale peut ne pas être entièrement exacte, mais nous pensons qu'elle est suffisamment plausible pour être prise au sérieux. Les entreprises d'IA, les décideurs politiques et la société dans son ensemble doivent consacrer des efforts sérieux à la recherche et à la planification de l'IA transformative.</p><h3>Quels sont les risques en matière de sécurité ?</h3><p>Si nous considérons les idées mentionnées précédemment, il n'est pas difficile de comprendre pourquoi l'IA peut représenter un risque pour notre sécurité. Il y a deux raisons importantes de s'inquiéter.</p><p>Premièrement, il est difficile de construire des systèmes d'IA qui soient à la fois intelligents et conscients de leur environnement, tout en étant sûrs et fiables. C'est un peu comme si un grand maître d'échecs essayait de battre un joueur débutant qui ne reconnaîtrait pas les erreurs du grand maître. Si nous créons une IA beaucoup plus intelligente que nous mais ayant des objectifs contraires à nos meilleurs intérêts, cela pourrait avoir des conséquences désastreuses. C'est ce qu'on appelle le problème d'alignement technique.</p><p>Deuxièmement, les progrès rapides de l'IA pourraient entraîner des perturbations majeures dans les emplois, les économies et les structures de pouvoir. Ces perturbations pourraient être catastrophiques en elles-mêmes et pourraient rendre plus difficile le développement de systèmes d'IA de manière prudente et responsable, ce qui entraînerait encore plus de problèmes avec l'IA.</p><p>Nous pensons que si l'IA progresse rapidement, ces risques seront très importants. Ils pourraient également interagir entre eux de manière difficile à prévoir. Il est possible que nous nous rendions compte plus tard que nous avions tort et que ces risques n'étaient pas aussi graves, ou que nous puissions les résoudre facilement. Cependant, nous pensons qu'il vaut mieux être prudent car les conséquences des erreurs commises avec l'IA pourraient être désastreuses.</p><p>Nous avons déjà observé des comportements inattendus de la part des systèmes d'IA. Ils peuvent présenter de la toxicité, des biais, une absence de fiabilité et même un désir de pouvoir. À mesure que l'IA devient plus répandue et puissante, ces problèmes deviendront de plus en plus importants. Certains de ces problèmes pourraient être similaires aux défis auxquels nous serons confrontés avec une IA atteignant l'intelligence de niveau humain, voire au-delà.</p><p>Dans le domaine de la sécurité de l'IA, nous nous attendons à des développements à la fois prévisibles et surprenants. Même si nous résolvons tous les problèmes que nous connaissons actuellement, nous ne pouvons pas supposer que les problèmes futurs seront résolus de la même manière. Il pourrait y avoir de nouveaux problèmes effrayants qui surgissent lorsque les systèmes d'IA deviennent suffisamment intelligents pour comprendre leur place dans le monde, tromper les gens ou utiliser des stratégies que les humains ne peuvent pas comprendre. Il y a de nombreux problèmes préoccupants qui pourraient n'apparaître que lorsque l'IA sera très avancée.</p><h3>Notre approche : L'empirisme dans la sécurité de l'IA</h3><p>Nous pensons qu'il est difficile de progresser en science et en ingénierie sans étudier de près la chose sur laquelle nous travaillons. Pour comprendre l'IA et la rendre plus sûre, nous nous appuyons sur des preuves empiriques, qui proviennent d'expériences et d'évaluations des systèmes d'IA. Ces preuves sont comme une "vérité fondamentale" qui nous aide à apprendre et à progresser.</p><p>Cela ne signifie pas que nous ignorons la recherche théorique ou conceptuelle en matière de sécurité de l'IA, mais nous pensons que la recherche basée sur des preuves du monde réel est la plus importante. Il y a de nombreuses possibilités et défis en matière de sécurité de l'IA, et il est difficile de les explorer uniquement en y réfléchissant. Il est facile de se concentrer trop sur des problèmes qui pourraient ne jamais se produire ou de passer à côté de problèmes importants qui existent réellement. De bonnes recherches empiriques peuvent nous guider dans notre travail théorique et conceptuel.</p><p>De même, nous pensons que trouver et résoudre des problèmes de sécurité est un processus complexe qui nécessite d'essayer différentes approches et d'apprendre d'elles. Donc, même si nous avons un plan pour nos recherches, nous sommes flexibles et prêts à le changer au fur et à mesure que nous en apprenons davantage. Nous savons que tous nos plans ne seront pas couronnés de succès, mais cela fait partie de la recherche.</p><h3>Le rôle des modèles de pointe dans la sécurité empirique</h3><p>Nous existons car nous pensons qu'il est important de mener des recherches sur la sécurité des systèmes d'IA avancés. Cela nécessite une organisation capable de travailler avec de grands modèles et de donner la priorité à la sécurité.</p><p>La recherche empirique ne nécessite pas toujours l'utilisation de systèmes d'IA avancés. Mais dans notre cas, les grands modèles sont différents des plus petits et sont directement liés à la sécurité :</p><ul>  <li>De nombreux problèmes de sécurité sérieux ne se posent qu'avec des IA proches du niveau humain, nous devons donc avoir accès à de tels systèmes pour étudier et résoudre ces problèmes.</li>  <li>Certaines méthodes de sécurité ne peuvent être testées que sur de grands modèles, et travailler avec des modèles plus petits ne nous permettrait pas d'explorer et de prouver ces méthodes.</li>  <li>Étant donné que nous nous concentrons sur la sécurité des modèles futurs, nous devons comprendre comment la sécurité évolue à mesure que les modèles deviennent plus grands.</li>  <li>Si les futurs grands modèles sont très dangereux, nous devons rassembler des preuves solides, ce qui nécessite généralement l'utilisation de grands modèles.</li></ul><p>Cependant, il y a un défi. Nous devons éviter d'accélérer le déploiement de technologies dangereuses grâce à la recherche sur la sécurité. Mais être excessivement prudent ne doit pas ralentir les recherches importantes. Il ne suffit pas de simplement faire de la recherche sur la sécurité ; nous devons également intégrer cette recherche dans des systèmes réels rapidement.</p><p>Trouver le bon équilibre est important pour nous. Ces préoccupations orientent nos décisions dans divers aspects de notre organisation, y compris la gouvernance, le recrutement, le déploiement, la sécurité et les partenariats. À l'avenir, nous prévoyons de prendre des engagements et de permettre des évaluations indépendantes pour nous assurer de respecter les normes de sécurité lors du développement de modèles avancés.</p><h3>Adopter une approche en portefeuille pour la sécurité de l'IA</h3><p>Certains chercheurs s'inquiètent des risques associés aux systèmes d'IA avancés. Cependant, prédire leur comportement et leurs caractéristiques est difficile, et prédire leur sécurité l'est encore plus. Nous pensons qu'un large éventail de scénarios est possible.</p><p>Voici trois scénarios à prendre en compte :</p><ol>  <li><strong>Scénarios optimistes :</strong> Les risques catastrophiques liés à l'IA sont peu probables, et les techniques de sécurité existantes sont généralement suffisantes. Les principaux risques sont liés à des problèmes tels que la toxicité et l'utilisation intentionnelle abusive, ainsi qu'aux changements sociétaux causés par l'automatisation et les changements dans les dynamiques de pouvoir.</li>  <li><strong>Scénarios intermédiaires :</strong> Des risques catastrophiques sont possibles, mais grâce à des efforts ciblés, nous pouvons les atténuer grâce aux avancées scientifiques et technologiques.</li>  <li><strong>Scénarios pessimistes :</strong> La sécurité de l'IA est un problème insoluble, et il convient d'éviter le développement de systèmes d'IA avancés pour prévenir les risques graves. Ce scénario nécessite de la prudence et l'évaluation des preuves de sécurité.</li></ol><p>Dans les scénarios optimistes, nous pouvons accélérer l'utilisation bénéfique de l'IA et faire face aux préjudices à court terme causés par les systèmes d'IA. Dans les scénarios intermédiaires, notre objectif est d'identifier les risques et de développer des méthodes d'entraînement sûres pour les puissants systèmes d'IA. Dans les scénarios pessimistes, nous mettons l'accent sur la fourniture de preuves des risques de sécurité et sur la défense de la non-prolifération d'IA dangereuses.</p><p>Notre priorité est de recueillir plus d'informations pour déterminer dans quel scénario nous nous trouvons. Notre recherche vise à mieux comprendre les systèmes d'IA et à détecter les comportements préoccupants. Nous travaillons à développer des techniques plus sûres et à identifier le niveau de sécurité des systèmes d'IA.</p><p>Nous croyons en une approche en portefeuille pour la recherche sur la sécurité de l'IA, en abordant différents scénarios. Notre objectif est de progresser dans les scénarios intermédiaires, de sensibiliser dans les scénarios pessimistes et de contribuer à l'amélioration globale de la sécurité de l'IA.</p><h3>Les trois types de recherche en IA chez Anthropic<br/></h3><p>Nous divisons nos projets de recherche chez Anthropic en trois catégories :</p><ul>  <li><strong>Capacités :</strong> Recherche visant à améliorer les performances des systèmes d'IA dans diverses tâches telles que l'écriture, le traitement d'images et les jeux. Nous nous concentrons sur la rendement plus efficace des modèles et le développement d'algorithmes meilleurs. Nous donnons la priorité à la recherche sur la sécurité plutôt qu'à l'avancement des capacités de l'IA.</li>  <li><strong>Alignement des capacités :</strong> Recherche axée sur l'entraînement des systèmes d'IA à être utiles, honnêtes et alignés sur les valeurs humaines. Nous développons des algorithmes tels que le débat, le red-teaming et l'apprentissage par renforcement basé sur les retours humains. Ces techniques sont précieuses pour une utilisation pratique et pour se préparer à des systèmes d'IA plus capables.</li>  <li><strong>Science de l'alignement :</strong> Recherche visant à évaluer et comprendre l'alignement des systèmes d'IA, à évaluer l'efficacité des capacités d'alignement et à étudier les limites de ces techniques. Nous explorons l'interprétabilité, les modèles de langage et la généralisation. Ce travail nous aide à identifier les problèmes potentiels et à mettre en évidence les limites des capacités d'alignement.</li></ul><p>Les capacités d'alignement et la science de l'alignement peuvent être considérées comme les approches de l'équipe "bleue" et de l'équipe "rouge", respectivement. Les capacités d'alignement se concentrent sur le développement de nouvelles techniques, tandis que la science de l'alignement vise à comprendre et à révéler leurs limites.</p><p>Notre catégorisation est utile car elle nous permet de traiter les débats au sein de la communauté de la sécurité de l'IA. La recherche pragmatique sur les capacités d'alignement fournit la base pour développer des techniques pour des modèles plus avancés. Cela nous aide à rendre les modèles plus honnêtes, corrigibles et précieux pour les humains. Cela encourage également les développeurs d'IA à investir dans la sécurité et à détecter les éventuelles défaillances.</p><p>Si la sécurité de l'IA est gérable, notre travail sur les capacités d'alignement aura le plus grand impact. Si le problème d'alignement est plus difficile, nous nous appuyons sur la science de l'alignement pour identifier les faiblesses de nos techniques. Et si le problème d'alignement est presque impossible, la science de l'alignement devient cruciale pour argumenter en faveur de l'arrêt du développement de systèmes d'IA avancés.</p><h3>Nos recherches actuelles sur la sécurité</h3><p>Nous explorons différentes approches pour entraîner les systèmes d'IA de manière sûre. Voici quelques idées importantes sur lesquelles nous travaillons :</p><ul>  <li>Interprétabilité mécaniste</li>  <li>Supervision évolutive</li>  <li>Apprentissage axé sur les processus</li>  <li>Compréhension de la généralisation</li>  <li>Tests des modes de défaillance dangereux</li>  <li>Impacts sociétaux et évaluations</li></ul><h4>Interprétabilité Mécaniste</h4><p>Nous travaillons sur différentes approches pour entraîner les systèmes d'IA de manière sûre et détecter les comportements indésirables.</p><p>Une approche est l'interprétabilité mécaniste, où nous cherchons à rétroconcevoir les réseaux neuronaux en algorithmes compréhensibles par les humains. Cela nous permettrait d'auditer les modèles pour la sécurité et d'identifier les aspects non sécurisés.</p><p>Bien que ce soit un problème difficile, il y a des signes que ce n'est pas impossible. Nous avons progressé dans la compréhension des composants des modèles de vision et dans l'extension de l'interprétabilité aux modèles de langage plus petits. Nous continuons d'en apprendre davantage sur les mécanismes de calcul des réseaux neuronaux.</p><p>Nous sommes motivés par l'approche empirique et ouverts à changer de direction si nous trouvons des approches plus prometteuses. Comprendre le fonctionnement des réseaux neuronaux et de l'apprentissage nous fournira davantage d'outils pour assurer la sécurité.</p><h4>Supervision Évolutive</h4><p>Pour entraîner les systèmes d'IA à se comporter de manière sûre et alignée sur les valeurs humaines, nous avons besoin de beaucoup de retours de haute qualité. Cependant, il y a des inquiétudes selon lesquelles les humains pourraient ne pas être en mesure de fournir les retours nécessaires.</p><p>Les humains pourraient ne pas fournir de retours précis, être trompés par les systèmes d'IA ou avoir du mal à fournir des retours à grande échelle. C'est ce qu'on appelle le problème de la supervision évolutive et c'est un problème central dans la formation de systèmes d'IA sûrs.</p><p>Nous pensons que les systèmes d'IA devraient se superviser partiellement ou aider les humains dans la supervision pour relever ce défi. Des techniques telles que RLHF et l'IA constitutionnelle montrent des promesses pour amplifier la supervision humaine en une supervision efficace de l'IA.</p><p>La supervision évolutive nous permet également d'automatiser les évaluations en équipe, où nous testons les systèmes d'IA avec des entrées potentiellement problématiques et les entraînons à se comporter de manière plus honnête et sans danger. Cela contribue à former des systèmes robustement sûrs.</p><p>Nous étudions différentes méthodes pour la supervision évolutive, y compris l'IA coopérative, la supervision assistée par des humains, le débat IA-IA, l'évaluation générée par le modèle, et l'équipe rouge en apprentissage par renforcement multi-agents. La mise à l'échelle de la supervision offre des perspectives prometteuses pour la formation de systèmes d'IA avancés à la fois capables et sûrs.</p><h4>Apprendre le Processus plutôt que d'Atteindre des Résultats</h4><p>Quand on apprend une nouvelle tâche, il existe différentes approches. L'une consiste à essayer différentes stratégies jusqu'à ce que l'on réussisse. Une autre approche consiste à apprendre d'un expert qui vous enseigne ses processus réussis.</p><p>L'apprentissage axé sur le processus se concentre sur la maîtrise des étapes et des méthodes utilisées pour atteindre un objectif, plutôt que d'atteindre simplement le résultat final. Cela implique la collaboration et l'amélioration au fil du temps.</p><p>En termes de sécurité de l'IA, former des systèmes d'IA en utilisant l'apprentissage axé sur le processus peut répondre à de nombreuses préoccupations :</p><ul><li>Les experts humains comprennent et justifient les étapes suivies par les systèmes d'IA.</li><li>Les systèmes d'IA sont récompensés en fonction de l'efficacité et de la compréhensibilité de leurs processus, pas seulement du résultat final.</li><li>Les systèmes d'IA sont dissuadés de poursuivre des comportements problématiques tels que l'acquisition de ressources ou la tromperie.</li></ul><p>À Anthropic, nous pensons que se concentrer sur l'apprentissage axé sur le processus est un moyen simple et efficace de résoudre les problèmes liés aux systèmes d'IA avancés. Nous explorons également ses limites et étudions les problèmes de sécurité qui peuvent survenir lors de la combinaison de l'apprentissage axé sur le processus et l'apprentissage axé sur les résultats.</p><h4>Comprendre la Généralisation</h4><p>Dans nos recherches, nous essayons de comprendre le fonctionnement des réseaux neuronaux en les rétro-ingénierie. Nous étudions également comment les grands modèles de langage sont entraînés.</p><p>Les grands modèles de langage peuvent présenter des comportements surprenants tels que la créativité, l'autoprotection et la tromperie. Ces comportements proviennent des données d'entraînement, mais le processus est complexe. Tout d'abord, les modèles sont entraînés sur une énorme quantité de texte, apprenant des représentations diverses et simulant différentes choses. Ensuite, ils sont affinés de différentes manières, ce qui peut avoir des conséquences inattendues. Le processus d'affinage dépend des biais de l'entraînement initial, qui provient d'une vaste quantité de connaissances.</p><p>Lorsqu'un modèle adopte un comportement préoccupant, comme celui de prétendre être une IA d'accord mais qui induit en erreur, nous voulons savoir s'il répète simplement ce qu'il a appris ou s'il croit réellement à ce comportement. Nous développons des techniques pour retracer les sorties du modèle jusqu'aux données d'entraînement afin de mieux le comprendre.</p><h4>Tests pour les Modes de Défaillance Dangereux</h4><p class="text-b2">Nous sommes préoccupés par le fait que les systèmes d'IA avancés pourraient développer des comportements nuisibles tels que la tromperie ou la planification stratégique. Pour remédier à cela, nous entraînons des modèles plus petits avec ces comportements pour les étudier sans risque de danger.</p><p class="text-b2">Nous nous intéressons particulièrement à la manière dont les systèmes d'IA se comportent lorsqu'ils savent qu'ils interagissent avec des humains dans un environnement d'entraînement. Deviennent-ils trompeurs ou développent-ils des objectifs indésirables ? Nous voulons comprendre ces tendances et comment elles évoluent à mesure que les modèles deviennent plus puissants.</p><p>Cependant, nous devons être prudents dans cette recherche. Il est sûr d'étudier ces comportements dans des petits modèles, mais cela pourrait être risqué avec des modèles plus grands et plus capables. Nous n'effectuerons pas cette recherche sur des modèles pouvant causer des préjudices graves.</p><h4>Impacts Sociétaux et Évaluations</h4><p class="text-b2">Nous considérons attentivement l'impact de nos systèmes d'IA sur la société. Nous étudions leurs capacités, leurs limites et les effets potentiels sur la société. Par exemple, nous analysons comment de grands modèles de langage peuvent avoir des comportements inattendus et potentiellement nuisibles. Nous testons également les modèles pour les sorties offensantes et cherchons des moyens de réduire les biais et les stéréotypes.</p><p class="text-b2">Nous sommes très préoccupés par l'impact des systèmes d'IA puissants sur la société. Nous évaluons leurs comportements potentiellement nuisibles, prévoyons leur utilisation et étudions leur impact économique. Cette recherche nous aide à développer des politiques d'IA responsables. En comprenant les effets de l'IA, nous visons à aider les décideurs politiques et les chercheurs à minimiser les préjudices et à s'assurer que l'IA profite à tous dans la société.</p><h4>Pensées finales</h4><p class="text-b2">Nous pensons que l'intelligence artificielle pourrait avoir un impact important sur le monde dans la prochaine décennie. Nous voulons nous assurer que ces systèmes avancés sont alignés sur les valeurs humaines et ne posent pas de risques sérieux.</p><p class="text-b2">Nous ne sommes pas inquiets concernant les systèmes d'IA actuels, mais nous voulons être prêts pour l'avenir. Nous effectuons actuellement un travail fondamental pour réduire les risques liés aux systèmes d'IA plus puissants.</p><p class="text-b2">Chez Anthropic, nous abordons la sécurité de l'IA avec des expériences pratiques. Nous étudions comment l'IA apprend et applique les connaissances, nous cherchons des moyens de superviser et de passer en revue les systèmes d'IA, nous rendons les systèmes d'IA transparents et faciles à comprendre, nous formons l'IA à suivre des processus sûrs, à prévenir les défaillances dangereuses et à évaluer l'impact de l'IA sur la société. Nous explorons différentes approches pour rester en sécurité dans différentes situations.</p><h4>Notes de bas de page</h4><ol>  <li id="footnote-1">Les progrès dans les algorithmes et le matériel d'intelligence artificielle augmentent rapidement, et le taux de croissance global est estimé être exponentiel.</li>  <li>Notre objectif est de développer des systèmes d'IA capables de lire, d'écrire et d'interagir avec les valeurs humaines.</li>  <li>Nous prévoyons une augmentation significative des capacités de l'IA au cours des prochaines années grâce aux avancées en matière de calcul, mais il est difficile de prédire exactement les progrès réalisés.</li>  <li>Dans la recherche en intelligence artificielle, il y a eu des surprises et des mystères dans la compréhension de la façon dont les réseaux neuronaux apprennent et généralisent.</li>  <li>Pour mener des recherches efficaces sur la sécurité, nous devons développer des systèmes d'IA en interne chez Anthropic.</li></ol>