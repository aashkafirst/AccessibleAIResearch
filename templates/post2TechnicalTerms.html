<h1> Charting a Path to AI Accountability </h1><h2>In Technical Terms</h2>
<p><br/></p>
<p>This week, Anthropic submitted <a href="https://cdn2.assets-servd.host/anthropic-website/production/images/Anthropic-NTIA-Comment.pdf">a response</a> to the National Telecommunications and Information Administration’s (NTIA) <a href="https://ntia.gov/issues/artificial-intelligence/request-for-comments">Request for Comment on AI Accountability</a>. Today, we want to share our recommendations as they capture some of Anthropic’s core AI policy proposals.</p>
<p>There is currently no robust and comprehensive process for evaluating today’s advanced artificial intelligence (AI) systems, let alone the more capable systems of the future. Our submission presents our perspective on the processes and infrastructure needed to ensure AI accountability. Our recommendations consider the NTIA’s potential role as a coordinating body that sets standards in collaboration with other government agencies like the <a href="https://www.anthropic.com/index/an-ai-policy-tool-for-today-ambitiously-invest-in-nist">National Institute of Standards and Technology (NIST)</a>.</p>
<p>In our recommendations, we focus on accountability mechanisms suitable for highly capable and general-purpose AI models. Specifically, we recommend:</p>
<ul><li><strong>Fund research to build better evaluations</strong><ul><li>Increase funding for AI model evaluation research. Developing rigorous, standardized evaluations is difficult and time-consuming work that requires significant resources. Increased funding, especially from government agencies, could help drive progress in this critical area.</li><li>Require companies in the near-term to disclose evaluation methods and results. Companies deploying AI systems should be mandated to satisfy some disclosure requirements with regard to their evaluations, though these requirements need not be made public if doing so would compromise intellectual property (IP) or confidential information. This transparency could help researchers and policymakers better understand where existing evaluations may be lacking.</li><li>Develop in the long term a set of industry evaluation standards and best practices. Government agencies like NIST could work to establish standards and benchmarks for evaluating AI models’ capabilities, limitations, and risks that companies would comply with.</li></ul></li><li><strong>Create risk-responsive assessments based on model capabilities</strong><ul><li>Develop standard capabilities evaluations for AI systems. Governments should fund and participate in the development of rigorous capability and safety evaluations targeted at critical risks from advanced AI, such as deception and autonomy. These evaluations can provide an evidence-based foundation for proportionate, risk-responsive regulation.</li><li>Develop a risk threshold through more research and funding into safety evaluations. Once a risk threshold has been established, we can mandate evaluations for all models against this threshold.<ul><li>If a model falls below this risk threshold, existing safety standards are likely sufficient. Verify compliance and deploy.</li><li>If a model exceeds the risk threshold and safety assessments and mitigations are insufficient, halt deployment, significantly strengthen oversight, and notify regulators. Determine appropriate safeguards before allowing deployment.</li></ul></li></ul></li><li><strong>Establish pre-registration for large AI training runs</strong><ul><li>Establish a process for AI developers to report large training runs ensuring that regulators are aware of potential risks. This involves determining the appropriate recipient, required information, and appropriate cybersecurity, confidentiality, IP, and privacy safeguards. </li><li>Establish a confidential registry for AI developers conducting large training runs to pre-register model details with their home country’s national government (e.g., model specifications, model type, compute infrastructure, intended training completion date, and safety plans) before training commences. Aggregated registry data should be protected to the highest available standards and specifications.</li></ul></li><li><strong>Empower third party auditors that are… </strong><ul><li><strong>Technically literate</strong> – at least some auditors will need deep machine learning experience;</li><li><strong>Security-conscious</strong> – well-positioned to protect valuable IP, which could pose a national security threat if stolen; and</li><li><strong>Flexible</strong> – able to conduct robust but lightweight assessments that catch threats without undermining US competitiveness.</li></ul></li><li><strong>Mandate external red teaming before model release</strong><ul><li>Mandate external red teaming for AI systems, either through a centralized third party (e.g., NIST) or in a decentralized manner (e.g., via researcher API access)  to standardize adversarial testing of AI systems. This should be a precondition for developers who are releasing advanced AI systems.</li><li>Establish high-quality external red teaming options before they become a precondition for model release. This is critical as red teaming talent currently resides almost exclusively within private AI labs.</li></ul></li><li><strong>Advance interpretability research</strong><ul><li>Increase funding for interpretability research. Provide government grants and incentives for interpretability work at universities, nonprofits, and companies. This would allow meaningful work to be done on smaller models, enabling progress outside frontier labs.</li><li>Recognize that regulations demanding interpretable models would currently be infeasible to meet, but may be possible in the future pending research advances.</li></ul></li><li><strong>Enable industry collaboration on AI safety via clarity around antitrust</strong><ul><li>Regulators should issue guidance on permissible AI industry safety coordination given current antitrust laws. Clarifying how private companies can work together in the public interest without violating antitrust laws would mitigate legal uncertainty and advance shared goals.</li></ul></li></ul>
<ul><li>Increase funding for AI model evaluation research. Developing rigorous, standardized evaluations is difficult and time-consuming work that requires significant resources. Increased funding, especially from government agencies, could help drive progress in this critical area.</li><li>Require companies in the near-term to disclose evaluation methods and results. Companies deploying AI systems should be mandated to satisfy some disclosure requirements with regard to their evaluations, though these requirements need not be made public if doing so would compromise intellectual property (IP) or confidential information. This transparency could help researchers and policymakers better understand where existing evaluations may be lacking.</li><li>Develop in the long term a set of industry evaluation standards and best practices. Government agencies like NIST could work to establish standards and benchmarks for evaluating AI models’ capabilities, limitations, and risks that companies would comply with.</li></ul>
<ul><li>Develop standard capabilities evaluations for AI systems. Governments should fund and participate in the development of rigorous capability and safety evaluations targeted at critical risks from advanced AI, such as deception and autonomy. These evaluations can provide an evidence-based foundation for proportionate, risk-responsive regulation.</li><li>Develop a risk threshold through more research and funding into safety evaluations. Once a risk threshold has been established, we can mandate evaluations for all models against this threshold.<ul><li>If a model falls below this risk threshold, existing safety standards are likely sufficient. Verify compliance and deploy.</li><li>If a model exceeds the risk threshold and safety assessments and mitigations are insufficient, halt deployment, significantly strengthen oversight, and notify regulators. Determine appropriate safeguards before allowing deployment.</li></ul></li></ul>
<ul><li>If a model falls below this risk threshold, existing safety standards are likely sufficient. Verify compliance and deploy.</li><li>If a model exceeds the risk threshold and safety assessments and mitigations are insufficient, halt deployment, significantly strengthen oversight, and notify regulators. Determine appropriate safeguards before allowing deployment.</li></ul>
<ul><li>Establish a process for AI developers to report large training runs ensuring that regulators are aware of potential risks. This involves determining the appropriate recipient, required information, and appropriate cybersecurity, confidentiality, IP, and privacy safeguards. </li><li>Establish a confidential registry for AI developers conducting large training runs to pre-register model details with their home country’s national government (e.g., model specifications, model type, compute infrastructure, intended training completion date, and safety plans) before training commences. Aggregated registry data should be protected to the highest available standards and specifications.</li></ul>
<ul><li><strong>Technically literate</strong> – at least some auditors will need deep machine learning experience;</li><li><strong>Security-conscious</strong> – well-positioned to protect valuable IP, which could pose a national security threat if stolen; and</li><li><strong>Flexible</strong> – able to conduct robust but lightweight assessments that catch threats without undermining US competitiveness.</li></ul>
<ul><li>Mandate external red teaming for AI systems, either through a centralized third party (e.g., NIST) or in a decentralized manner (e.g., via researcher API access)  to standardize adversarial testing of AI systems. This should be a precondition for developers who are releasing advanced AI systems.</li><li>Establish high-quality external red teaming options before they become a precondition for model release. This is critical as red teaming talent currently resides almost exclusively within private AI labs.</li></ul>
<ul><li>Increase funding for interpretability research. Provide government grants and incentives for interpretability work at universities, nonprofits, and companies. This would allow meaningful work to be done on smaller models, enabling progress outside frontier labs.</li><li>Recognize that regulations demanding interpretable models would currently be infeasible to meet, but may be possible in the future pending research advances.</li></ul>
<ul><li>Regulators should issue guidance on permissible AI industry safety coordination given current antitrust laws. Clarifying how private companies can work together in the public interest without violating antitrust laws would mitigate legal uncertainty and advance shared goals.</li></ul>
<p>We believe this set of recommendations will bring us meaningfully closer to establishing an effective framework for AI accountability. Doing so will require collaboration between researchers, AI labs, regulators, auditors, and other stakeholders. Anthropic is committed to supporting efforts to enable the safe development and deployment of AI systems. Evaluations, red teaming, standards, interpretability and other safety research, auditing, and strong cybersecurity practices are all promising avenues for mitigating the risks of AI while realizing its benefits.</p>
<p>We believe that AI could have transformative effects in our lifetime and we want to ensure that these effects are positive. The creation of robust AI accountability and auditing mechanisms will be vital to realizing this goal. We are grateful for the chance to respond to this Request For Comment. </p>
<p>You can read our submission in full <a href="https://cdn2.assets-servd.host/anthropic-website/production/images/Anthropic-NTIA-Comment.pdf">here</a>.</p>
