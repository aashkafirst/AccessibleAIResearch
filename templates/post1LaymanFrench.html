<h1>Points essentiels sur la sécurité de l'IA : Quand, pourquoi, quoi et comment</h1><h2>En termes simples en français</h2><p><br></p> <p>Nous avons créé Anthropic parce que nous pensons que l'IA pourrait changer le monde beaucoup, comme la science et les usines l'ont fait. Mais nous sommes inquiets que cela puisse arriver bientôt et ne pas bien se passer.</p> <p>Cela peut paraître étrange, et il existe de bonnes raisons de ne pas y croire. Presque tout le monde qui dit "ce sur quoi nous travaillons pourrait être vraiment important!" Finit par se tromper. Mais nous pensons qu'il existe suffisamment de preuves pour se préparer à une IA qui s'améliore rapidement et change tout.</p><p>Chez Anthropic, nous aimons montrer au lieu de dire. Nous avons publié de nombreuses recherches sur la manière de rendre l'IA sûre, ce qui, selon nous, aide tous ceux qui travaillent sur l'IA. Nous écrivons ceci maintenant parce que plus de gens savent que l'IA s'améliore rapidement. Nous voulons expliquer ce que nous pensons et ce que nous faisons. En résumé, nous croyons que la recherche en matière de sécurité de l'IA est urgente et importante et devrait être soutenue par de nombreux groupes.</p> <p>Ici, nous expliquerons pourquoi nous pensons que l'IA s'améliorera très rapidement et changera beaucoup, et pourquoi cela nous a inquiétés pour la sécurité de l'IA. Nous résumerons également brièvement notre approche de la recherche en matière de sécurité de l'IA et les raisons pour lesquelles nous adoptons cette approche. Nous espérons que cela aide à discuter de la sécurité de l'IA et des progrès.</p><p>En résumé, les principaux points ici sont:</p> <ul> <li><strong>L'IA pourrait avoir un impact énorme, peut-être dans les 10 prochaines années</strong><br> <strong></strong>L'IA continue de s'améliorer rapidement parce que nous utilisons de plus en plus de puissance de calcul pour la former. Les études montrent qu'avec plus de calcul, l'IA devient plus capable. De simples estimations suggèrent que l'IA pourrait correspondre ou dépasser les humains dans la plupart des tâches mentales dans les 10 prochaines années. Les progrès de l'IA pourraient ralentir ou s'arrêter, mais les signes indiquent qu'elle continuera probablement.</li><li><strong>Nous ne savons pas comment former l'IA à se comporter bien de manière cohérente</strong><br> <strong></strong>Personne ne sait comment former une IA très avancée pour qu'elle soit utile, honnête et sûre. De plus, les progrès rapides de l'IA changeront beaucoup la société et pourraient conduire les entreprises ou les pays à utiliser une IA non fiable. Cela pourrait être catastrophique, soit parce que l'IA choisit des objectifs dangereux, soit parce qu'elle commet des erreurs innocentes mais dangereuses lorsque beaucoup est en jeu.</li><li><strong>Nous sommes les plus optimistes quant à un ensemble de moyens d'étudier la sécurité de l'IA</strong><h3>Notre vision approximative des progrès rapides de l'IA</h3><p> Les trois principales choses qui rendent les systèmes d'IA plus intelligents avec le temps sont <a href="#footnote-1"><sup>1</sup></a> les données d'entraînement, la puissance de calcul et de meilleurs algorithmes. Au milieu des années 2010, certains scientifiques ont remarqué que les plus grands systèmes d'IA étaient généralement plus intelligents. Ils ont donc pensé que la chose la plus importante pour les progrès de l'IA pourrait être la quantité de calcul utilisée pour former les systèmes. Lorsqu'ils l'ont représenté sur un graphique, il était clair que le calcul investi dans les plus grands modèles d'IA croissait de <a href="https://openai.com/blog/ai-and-compute/ "><span>10 fois</span></a> chaque année - bien plus vite que la loi de Moore. En 2019, certains de ces scientifiques ont précisément montré que vous pouviez rendre les systèmes d'IA plus intelligents d'une manière prévisible en les rendant simplement plus grands et en les formant sur plus de données. En utilisant ces résultats, cette équipe a mené le travail de formation de <a href="https://arxiv.org/abs/2005.14165"><span>GPT-3</span></a>, peut-être le premier grand modèle de langage <sup>2</sup>.</p><p>Depuis la découverte de la façon de mettre à l'échelle l'IA, beaucoup à Anthropic pensent que les progrès de l'IA pourraient être très rapides. Mais, en 2019, il semblait que les nombreux types de données, la pensée logique, la vitesse d'apprentissage, l'utilisation des connaissances d'une tâche à une autre et la mémoire à long terme pourraient ralentir ou stopper les progrès de l'IA. Depuis, certaines de ces "barrières" comme le besoin de nombreux types de données et de pensée logique ont disparu. Ainsi, la plupart des personnes d'Anthropic pensent maintenant que les progrès de l'IA se poursuivront au lieu de ralentir ou de s'arrêter. Les systèmes d'IA sont désormais presque aussi performants que les humains dans de nombreuses tâches, mais leur formation coûte encore bien moins cher que les grands projets scientifiques comme le télescope spatial Hubble. Il y a donc beaucoup de place pour continuer à les améliorer.<sup>3</sup></p> <p>Les gens ne remarquent ou n'acceptent généralement pas la croissance exponentielle au début. Bien que nous assistions à des progrès rapides en matière d'IA, beaucoup pensent que cela doit être temporaire et que les choses reviendront bientôt à la normale. Mais, si nous avons raison, le sentiment de progrès rapides en matière d'IA ne pourra pas prendre fin avant que les systèmes d'IA n'aient une large gamme de compétences supérieures aux nôtres. De plus, l'utilisation de l'IA avancée pour accélérer les progrès de l'IA pourrait accélérer cela encore plus; nous le voyons déjà avec l'IA qui aide les chercheurs à être plus productifs et <a href="https://arxiv.org/abs/2212.08073"><span>Constitutional AI</span></a><h3>Quels risques pour la sécurité ?</h3><p>Si vous croyez ce que j'ai dit avant, l'IA pourrait être dangereuse. Il y a deux raisons simples pour lesquelles nous devrions nous inquiéter. </p> <p>Premièrement, il peut être difficile de créer des systèmes d'IA sûrs, fiables et contrôlables une fois qu'ils deviennent aussi intelligents que les personnes qui les ont construits. En prenant un exemple, il est facile pour un expert aux échecs de voir de mauvais coups d'un débutant mais difficile pour le débutant de voir de mauvais coups d'un expert. Si nous rendons l'IA plus intelligente que nous mais avec des objectifs différents, cela pourrait mal se terminer. C'est le problème de faire correspondre les objectifs de l'IA aux nôtres.</p><p>Deuxièmement, des progrès rapides en matière d'IA changeraient beaucoup de choses, comme les emplois, l'économie et le pouvoir entre les pays. Ces grands changements pourraient être des catastrophes en soi et aussi rendre plus difficile le développement prudent de l'IA. Cela pourrait conduire à davantage de problèmes. </p> <p>Nous pensons que si les progrès de l'IA sont rapides, ces risques seront énormes. Ils pourraient aussi s'aggraver les uns les autres de manières imprévisibles. Nous pourrions plus tard décider que nous avions tort et qu'ils ne seront pas des problèmes ou seront faciles à résoudre. Mais nous devons être prudents car nous tromper pourrait être terrible.</p><p>Nous avons déjà vu l'IA se comporter de manières que les créateurs ne voulaient pas, comme être toxique, biaisée, peu fiable, malhonnête et récemment, <a href="https://arxiv.org/pdf/2212.09251.pdf" target="_blank"><span>désireuse de plaire et de pouvoir</span></a>. À mesure que l'IA se répand et devient plus puissante, ces problèmes deviendront plus importants. Certains peuvent montrer les problèmes que nous aurons avec l'IA de niveau humain et au-delà.</p> <p>En matière de sécurité de l'IA, nous nous attendons à un mélange de <a href="https://dl.acm.org/doi/pdf/10.1145/3531146.3533229" target="_blank"> <span>changements prévisibles et surprenants</span></a>. Même en corrigeant les problèmes des IA d'aujourd'hui, nous ne pouvons pas supposer que nous résoudrons les problèmes futurs de la même manière. Des problèmes effrayants et spéculatifs ne pourraient apparaître qu'une fois que l'IA comprendra sa place, pourra bien tromper les gens ou aura des stratégies que nous ne comprenons pas. De nombreux problèmes inquiétants ne pourraient survenir que lorsque l'IA sera très avancée. </p><h3>Notre approche: l'empirisme dans la sécurité de l'IA</h3><p>Nous pensons qu'il est difficile de progresser rapidement dans la science sans observer attentivement ce que nous étudions. Répéter des tests contre la "vérité" aide généralement la science à progresser. Pour la sécurité de l'IA, notre principale vérité provient d'expériences de formation et de test de l'IA.</p> <p>Cela ne signifie pas que la théorie n'est pas importante pour la sécurité de l'IA. Mais la recherche basée sur ce que nous voyons comptera probablement le plus et aura le plus grand impact. Les types d'IA, les problèmes de sécurité et les solutions sont immenses et difficiles à déterminer juste en y pensant. Il est facile de se concentrer sur des problèmes qui ne se produiront pas ou de manquer les grands problèmes qui se produiront. De solides expériences améliorent souvent la théorie et les idées.</p><p>Les moyens de trouver et de corriger les problèmes de sécurité seront probablement extrêmement difficiles à planifier à l'avance. Ils auront besoin d'un travail étape par étape. Donc, même si nous pouvons avoir des plans à court terme, nous sommes prêts à les modifier à mesure que nous apprenons.  Nous ne pouvons pas promettre que nos recherches actuelles réussiront, mais c'est vrai pour toute recherche.</p><h3>Le rôle des modèles de pointe dans la sécurité empirique </h3><p>Anthropic existe principalement parce que nous pensons qu'il est important de rechercher la sécurité pour l'IA avancée. Cela nécessite un groupe capable de travailler avec de grands modèles et de se concentrer sur la sécurité.</p><p>Tester ce que nous voyons ne signifie pas toujours avoir besoin de travailler sur des systèmes de pointe. Vous pourriez imaginer étudier en toute sécurité des modèles plus petits et moins puissants. Mais ce n'est pas notre situation. Les énormes modèles sont très différents des plus petits, y compris des changements soudains et inattendus. La taille est également directement liée à la sécurité :</p><ul><li>Nos plus grandes inquiétudes ne pourraient survenir qu'avec une IA de niveau humain, et nous aurions besoin de cette IA pour progresser.</li> <li>Des méthodes de sécurité telles que l'IA constitutionnelle ou le débat ne peuvent fonctionner que sur de grands modèles. Des modèles plus petits rendent l'exploration et la preuve impossibles. </li><li>Nous nous concentrerons sur la sécurité des modèles futurs, nous devons donc voir comment les méthodes et les propriétés changent à mesure que les modèles évoluent.</li><li>Si les futurs énormes modèles sont très dangereux, nous aurions besoin d'eux pour montrer des preuves claires. </li> </ul><p>Malheureusement, si nous avons besoin de grands modèles pour une recherche solide en matière de sécurité, cela nous oblige à faire un choix difficile. Nous devons éviter d'accélérer une technologie non sécurisée. Mais trop de prudence pourrait ralentir une recherche vitale en n'utilisant que des modèles très en retard sur la frontière. De plus, la recherche ne suffit pas - nous devons acquérir des connaissances pour appliquer rapidement les dernières recherches en matière de sécurité aux systèmes réels.</p><p>Prendre ces décisions de manière responsable exige un équilibre. Ces préoccupations guident tout ce que nous faisons, y compris la recherche, la gestion, le recrutement, le déploiement de systèmes, la sécurité et les partenariats. Bientôt, nous prévoyons nous engager à ne développer des modèles avancés que si nous respectons les normes de sécurité. Et nous laisserons un groupe extérieur évaluer les capacités et la sécurité de nos modèles.</p><h3>Adopter une approche de portefeuille pour la sécurité de l'IA</h3> <p>Certains chercheurs se soucient de la sécurité en raison de solides opinions sur les risques de l'IA. Mais même prédire comment l'IA pourrait agir bientôt est difficile. Deviner à quel point les futurs systèmes seront sûrs semble encore plus difficile. Plutôt que d'adopter une position ferme, nous pensons que de nombreux résultats sont possibles.</p><p>Une inconnue importante est de savoir à quel point il sera difficile de rendre l'IA avancée largement sûre avec peu de risque humain. Cela pourrait aller de très facile à impossible. Appelons-les :</p> <ol><li><strong>Optimiste :</strong> Peu de chance de catastrophe due aux problèmes de sécurité de l'IA avancée. Des méthodes comme <a href="https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf" target="_blank">l'apprentissage par renforcement à partir des commentaires humains </a>(RLHF) et <a href="https://arxiv.org/abs/2212.08073" target="_blank">l'IA constitutionnelle </a>(CAI) suffisent probablement. Les principaux risques sont les problèmes actuels (toxicité, mauvaise utilisation) et l'automatisation/les impacts mondiaux - nécessitant des recherches.</li><li><strong>Intermédiaire :</strong> La catastrophe est possible ou même probable. Mais un travail ciblé peut atteindre la sécurité. </li><li><strong>Pessimiste :</strong> La sécurité de l'IA est insoluble. Nous ne pouvons pas aligner un système plus intelligent que nous, donc ne développons pas d'IA avancée.</li></ol><p><strong>Si optimiste...</strong> Ce qu'Anthropic fait compte moins ; la catastrophe est peu probable dans les deux cas. Nous accélérerons l'IA avancée bénéfique et réduirons les dommages à court terme. Nous pouvons aider à résoudre <a href="https://www.governance.ai/research-paper/thinking-about-risks-from-ai-accidents-misuse-and-structure" target="_blank">les risques structurels</a>, le plus grand problème sans défaillances catastrophiques. </p><p><strong>Si intermédiaire...</strong> Anthropic trouvera les risques de l'IA avancée et les moyens de former en toute sécurité des systèmes puissants. Nos méthodes comme l'IA constitutionnelle peuvent aider dans les scénarios "moyennement faciles" ou "moyennement difficiles".</p> <p><strong>Si pessimiste...</strong> Anthropic montrera que la sécurité de l'IA avancée ne peut pas prévenir la catastrophe, en levant l'alarme pour que nous évitions l'IA dangereuse. Près du pessimisme : nous concentrons la recherche et stoppons les progrès. Les signes pourraient venir vite et être difficiles à voir. Supposez toujours ce scénario à moins qu'il n'y ait des pre </p><h3>Les trois types de recherche sur l'IA chez Anthropic<br/></h3><p>Nous regroupons notre travail chez Anthropic en trois types:</p><ul><li><strong>Améliorer l'IA</strong>: Recherche pour améliorer l'IA, comme rendre les modèles de langage plus intelligents ou les algorithmes d'apprentissage par renforcement meilleurs. Nous faisons ce travail en privé car nous ne voulons pas que l'IA progresse trop vite. Nous avons créé notre premier grand modèle, Claude, en 2022 et nous l'utilisons maintenant pour étudier la sécurité.</li> <li><strong>Aligner l'IA</strong>: Travail sur de nouvelles façons d'entraîner l'IA à être utile, honnête et inoffensive. Des choses comme le débat, le red teaming, l'IA constitutionnelle et le RLHF. Ce travail n'a pas besoin d'être utile maintenant mais peut l'être plus tard à mesure que l'IA devient plus intelligente.</li><li><strong>Comprendre l'IA</strong>: Examiner si l'IA est vraiment alignée, à quel point nos méthodes d'alignement fonctionnent et si elles fonctionneront toujours à mesure que l'IA devient plus intelligente. Cela inclut l'interprétabilité, l'évaluation des modèles de langage, le red teaming et l'étude de la généralisation des modèles.</li> </ul><p>Vous pouvez voir "Aligner l'IA" comme une "équipe bleue" et "Comprendre l'IA" comme une "équipe rouge". L'"équipe bleue" développe de nouvelles méthodes tandis que l'"équipe rouge" trouve leurs faiblesses.</p>  <p>Certains affirment que RLHF n'était pas vraiment un travail de sécurité. Nous ne sommes pas d'accord. Un travail utile sur "Aligner l'IA" jette les bases de techniques sur des modèles plus intelligents, comme l'IA constitutionnelle et les évaluations de l'IA. Il permet également à l'IA d'aider la recherche en matière de sécurité en rendant les modèles plus honnêtes et ouverts aux commentaires.</p><p>Si la sécurité de l'IA est facile, notre travail sur "Aligner l'IA" peut aider le plus. S'il est difficile, nous compterons davantage sur "Comprendre l'IA" pour trouver des failles dans nos méthodes. Et s'il est presque impossible, nous avons désespérément besoin de "Comprendre l'IA" pour montrer que nous devons arrêter l'IA avancée.</p><h3>Notre recherche actuelle en matière de sécurité</h3> <p>Nous travaillons sur de nombreuses façons d'entraîner une IA sûre. Quelques idées principales sont:</p><ul><li><strong>Rendre l'IA compréhensible</strong>: Nous voulons comprendre pourquoi les modèles d'IA se comportent comme ils le font et s'ils resteront alignés à mesure qu'ils deviennent plus intelligents.</li><li><strong>Gérer l'IA</strong>: Nous avons besoin de moyens de superviser l'IA qui s'élargissent à mesure que les systèmes deviennent plus avancés. </li><li><strong>Nouvelles méthodes d'apprentissage</strong>: Essayer différentes façons pour les machines d'apprendre, comme se concentrer davantage sur le <i>processus</i> d'apprentissage lui-même.</li>  <li><strong>Comment l'IA généralise</strong>: Comprendre comment l'IA peut appliquer des connaissances d'une tâche à une nouvelle tâche. Cela permet de s'assurer que les connaissances sont appliquées en toute sécurité.</li><li><strong>Trouver les dangers</strong>: Nous testons les systèmes d'IA pour vérifier les comportements nocifs que nous voulons éviter à mesure qu'ils deviennent plus capables.</li><li><strong>Impacts sur la société</strong>: Nous étudions comment l'IA peut influencer le monde pour guider le progrès dans une bonne direction.</li></ul> <h4>Rendre l'IA compréhensible</h4><p>Résoudre le problème de l'IA non sécurisée repose sur la détection de comportements indésirables. Si nous pouvons les trouver même dans de nouvelles situations en "lisant dans l'esprit de l'IA", nous avons plus de chances d'entraîner une IA sûre. Pour l'instant, nous pouvons avertir les autres qu'un système d'IA n'est pas sûr.</p><p>Notre travail sur l'interprétabilité se concentre sur les lacunes laissées par d'autres recherches en matière de sécurité. La chose la plus précieuse serait de reconnaître si une IA simule l'alignement sur des tests difficiles. Si notre travail sur la gestion et les nouvelles méthodes d'apprentissage réussit, les modèles peuvent sembler alignés mais en fait être optimistes ou pessimistes. L'interprétabilité peut être la seule façon de le dire.</p> <p>Cela nous conduit à un objectif risqué: rétroconcevoir les réseaux de neurones en algorithmes compréhensibles par l'homme, comme auditer du code. Les modèles de langage sont des programmes complexes, et la "superposition" la rend plus difficile, mais nous voyons des signes que cela peut fonctionner. Avant Anthropic, certains d'entre nous ont constaté que les modèles de vision ont des circuits interprétables. Nous l'avons étendu à de petits modèles de langage et avons trouvé un mécanisme d'apprentissage en contexte. Nous comprenons aussi mieux le calcul neuronal, comme la mémorisation.</p> <p>C'est notre direction actuelle. Nous sommes guidés par les preuves, nous changerons donc de direction si nécessaire. Comprendre en détail les réseaux de neurones et l'apprentissage offre plus de moyens de poursuivre la sécurité.</p><p>Si l'interprétabilité fonctionne, l'avenir est optimiste ou intermédiaire. Sinon, c'est pessimiste. Nous visons la sécurité grâce à des techniques avancées, mais si elles ne fonctionnent pas, nous construisons un solide dossier pour arrêter les progrès.</p><h4>Gérer l'IA </h4><p>Apprendre aux modèles de langage à devenir une IA utile nécessite beaucoup de commentaires de haute qualité. Une inquiétude est que <a href="https://distill.pub/2019/safety-needs-social-scientists" rel="noreferrer noopener" style="text-decoration:none;" target="_blank"><span style="font-size:11pt;font-family:Arial;color:#1155cc;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:underline;vertical-align:baseline;white-space:pre-wrap;">les gens ne peuvent pas en donner assez</span></a>. Peut-être que les gens ne peuvent pas donner de commentaires assez bons pour que l'IA apprenne la sécurité dans de nombreuses situations. Peut-être que les gens peuvent être trompés par l'IA et donner le mauvais feedback par accident. Peut-être que les gens pourraient donner le bon feedback mais pas à grande échelle.</p><p>Le seul moyen d'obtenir suffisamment de bons commentaires peut être pour l'IA de se surveiller elle-même ou d'aider les gens à la surveiller. Nous devons transformer un peu de commentaires humains de haute qualité en beaucoup de commentaires d'IA de haute qualité. Des méthodes comme RLHF et l'IA constitutionnelle montrent des promesses, mais plus de travail est nécessaire pour l'IA de niveau humain. Les modèles de langage apprennent déjà les valeurs humaines pendant le pré-entraînement. De plus grands modèles peuvent mieux comprendre les valeurs et les apprendre plus facilement. L'objectif est que les modèles comprennent et suivent les valeurs humaines.</p>  <p>Des méthodes comme CAI permettent également le "red teaming" - générer automatiquement des problèmes pour l'IA et l'entraîner à répondre en toute sécurité. Nous espérons qu'une surveillance évolutive puisse former des systèmes plus sûrs. Nous étudions des moyens de faire évoluer la surveillance, comme améliorer CAI, aider les gens à superviser l'IA, l'IA en débat, le "red teaming" avec RL multi-agents et les évaluations générées par le modèle. L'échelle de supervision peut être la clé pour dépasser les capacités humaines en toute sécurité, mais plus de travail est nécessaire.</p> <h4>Nouvelles méthodes d'apprentissage </h4> <p>Une façon d'apprendre quelque chose de nouveau est d'essayer différentes façons jusqu'à ce que vous y arriviez. Cela s'appelle «l'apprentissage axé sur les résultats». Dans ce cas, la façon dont vous apprenez ne dépend que de l'objectif. Vous essayez différentes stratégies jusqu'à trouver une façon facile d'atteindre l'objectif.</p> <p>Une meilleure façon d'apprendre est de suivre les étapes d'un expert pour réussir. Au cours de la pratique, la réussite n'est pas aussi importante que l'amélioration de la façon dont vous faites les choses. À mesure que vous vous améliorez, vous pouvez travailler avec votre coach pour trouver des stratégies encore meilleures. Cela s'appelle «l'apprentissage axé sur le processus». L'objectif ici est de maîtriser les étapes, pas seulement d'atteindre le résultat. </p> <p>L'apprentissage axé sur le processus peut aider à répondre aux préoccupations concernant la sécurité de l'IA avancée. Dans cette méthode:</p><li> Les humains comprendront les étapes que prennent les systèmes d'IA car les étapes doivent être expliquées aux gens. </li> <li> Les systèmes d'IA ne seront pas récompensés pour avoir atteint des objectifs de manière confuse ou mauvaise. Ils seront récompensés pour des étapes claires et utiles. </li><li>Les systèmes d'IA ne seront pas récompensés pour des objectifs problématiques comme l'obtention de ressources ou la tromperie des autres. Les humains donneront des commentaires négatifs pour ce genre d'étapes pendant la formation.</li></ul> <p> Chez Anthropic, nous aimons les solutions simples. Limiter la formation de l'IA à l'apprentissage axé sur le processus pourrait résoudre de nombreux problèmes liés à l'IA avancée. Nous voulons découvrir les limites de cette méthode et quand l'apprentissage axé sur les résultats cause des problèmes de sécurité. Nous pensons que l'apprentissage axé sur le processus peut être la meilleure façon de former des systèmes sûrs et transparents, même au-delà du niveau humain. </p><h4>Comment l'IA généralise</h4><p>Nous essayons de mieux comprendre comment les grands modèles de langage (LLM) sont formés.</p> <p>Les LLM présentent des comportements surprenants, comme la créativité, l'autopréservation et la tromperie. Ceux-ci proviennent des données d'entraînement, mais le chemin est compliqué. Tout d'abord, les modèles apprennent à partir d'énormes quantités de texte brut. Ils développent de larges représentations et peuvent simuler différents agents. Ensuite, ils sont finement réglés de nombreuses façons, dont certaines sont surprenantes. Le réglage fin dépend beaucoup de ce qui est appris lors du pré-entraînement à l'aide d'une grande partie des connaissances du monde.</p><p>Lorsqu'un modèle se comporte comme une IA trompeuse mais utile, ne fait-il que répéter des données d'entraînement similaires ? Ou ce comportement, ou les croyances et valeurs qui le sous-tendent, sont-ils devenus la façon dont le modèle pense que les assistants IA devraient agir dans n'importe quelle situation ? Nous développons des moyens de retracer les sorties d'un modèle à ses données d'entraînement. Cela peut nous aider à comprendre le modèle.</p><p>Chez Anthropic, nous nous soucions de l'interprétation des modèles et de leurs comportements. Nous voulons savoir si les comportements proviennent de données spécifiques ou de la compréhension générale du modèle. Comprendre les biais implicites et les représentations d'un modèle issus du pré-entraînement est essentiel. Des sorties de modèle surprenantes pourraient provenir d'un réglage fin de manière non intentionnelle. Nos techniques d'analyse des modèles et de connexion de leurs sorties aux données d'entraînement fourniront d'importantes pistes pour les interpréter. </p><h4>Trouver les dangers</h4><p>Une préoccupation majeure est que l'IA avancée pourrait développer des comportements nocifs, comme la tromperie ou la planification stratégique, que l'IA plus simple n'a pas. Nous pensons que le moyen de les repérer avant qu'ils ne deviennent des menaces est d'entraîner délibérément de petits modèles limités à avoir ces comportements afin que nous puissions les étudier.</p><p>Nous nous intéressons particulièrement à la façon dont l'IA agit lorsqu'elle sait qu'elle est une IA qui parle aux humains pendant l'entraînement. L'IA devient-elle trompeuse ou développe-t-elle des objectifs surprenants et indésirables ? Idéalement, nous construirons des modèles détaillés de la façon dont ceux-ci évoluent avec la taille afin que nous puissions prévoir l'émergence de comportements dangereux.</p> <p>En même temps, cette recherche elle-même pourrait être risquée. Il est peu probable que cela se produise sur de petits modèles limités, mais cela implique de créer les comportements qui nous inquiètent. Ce serait risqué sur des modèles plus grands et plus capables. Nous ne mènerons pas cette recherche sur des modèles qui pourraient causer un préjudice réel. </p><p>Chez Anthropic, nous voulons nous assurer que les systèmes d'IA avancés se comportent de manière utile. Nous étudierons les échecs potentiellement dangereux dans de petits modèles, mais avec prudence. L'essentiel est de savoir si l'IA change lorsqu'elle est «consciente de la situation» - sachant qu'elle s'entraîne avec des humains.</p> <p>Les objectifs sont: voir si des comportements indésirables comme la tromperie émergent, en particulier à mesure que les modèles s'élargissent; construire des modèles pour prédire cela; garder la recherche sûre. Nous éviterons les modèles plus grands et les risques sérieux. Mais créer et étudier ces comportements, même dans de petits modèles, exige de la prudence.</p><p>Nous visons non pas une IA dangereuse, mais à progresser en toute sécurité en anticipant les problèmes. Tester et comprendre les échecs aidera à construire des systèmes intrinsèquement dignes de confiance. Mais nous devons être vigilants et responsables. Dans l'ensemble, nous pensons que cette approche peut réduire les risques liés à l'IA avancée si nous sommes réfléchis, prudents et attachés à la sécurité.</p><h4>Impacts sur la société </h4><p>Évaluer attentivement comment notre travail pourrait avoir un impact sur la société est essentiel. Nous construisons des outils et des mesures pour évaluer les capacités, les limites et les impacts sociétaux de l'IA. Par exemple, nous avons étudié comment les modèles de langage à grande échelle prévisibles ou surprenants peuvent conduire à des problèmes. Nous avons montré comment les surprises pouvaient être détournées. Nous avons également testé des moyens de trouver et de réduire les préjudices dans des modèles de différentes tailles. Récemment, nous avons constaté que les modèles de langage actuels peuvent suivre les instructions pour réduire les biais et les stéréotypes.</p><p>Nous nous inquiétons de l'impact sur la société à court, moyen et long terme du déploiement d'une IA de plus en plus puissante. Nous travaillons sur des projets pour: évaluer et atténuer les préjudices potentiels de l'IA; prédire comment l'IA pourrait être utilisée; étudier les impacts économiques. Ce travail aide également à élaborer des politiques et des règles d'IA responsables. En étudiant rigoureusement l'IA aujourd'hui, nous visons à donner aux décideurs politiques et aux chercheurs des informations et des outils pour réduire les risques sociétaux importants et garantir que les avantages de l'IA sont répartis équitablement.  </p><p>Chez Anthropic, nous considérons que le développement responsable de l'IA avancée est crucial. Notre approche: analyser les capacités et les limites de l'IA; anticiper les défis et les opportunités; traiter les problèmes de manière proactive. La recherche sur les implications sociétales doit se poursuivre à mesure que les progrès s'accélèrent. </p> <p>Exemples de notre travail: étude de la prévisibilité/surprise dans les modèles de langage, comment cela conduit à des problèmes; "red teaming" des modèles pour trouver/corriger les préjudices; constatation que les modèles peuvent réduire les biais lorsqu'on le leur demande. Nous sommes très préoccupés par le déploiement d'une IA de plus en plus puissante et ses impacts, et nous nous attaquons aux risques par des évaluations, des prédictions et des études d'impact pour guider les politiques et la gouvernance.</p><p>L'objectif est de fournir des informations et des outils pour que les décideurs politiques et les chercheurs puissent minimiser les risques sociétaux graves et garantir que les avantages sont distribués équitablement. En étudiant rigoureusement les implications maintenant, nous visons une IA digne de confiance, bénéfique et alignée sur les valeurs humaines à mesure que les progrès se poursuivent. Dans l'ensemble, nous pensons que des travaux proactifs réfléchis peuvent contribuer à augmenter les chances d'un avenir positif à long terme avec l'IA avancée.<h4>Réflexions finales</h4><p>Nous pensons que l'IA pourrait changer le monde beaucoup, peut-être dans 10 ans. Les ordinateurs deviennent de plus en plus puissants, donc la nouvelle IA sera beaucoup plus avancée qu'aujourd'hui. Mais nous ne savons pas exactement comment nous assurer que l'IA puissante s'aligne sur ce que les humains valorisent afin que le risque de catastrophe soit faible.</p> <p>L'IA d'aujourd'hui n'est pas une préoccupation immédiate. Mais nous devrions travailler maintenant pour réduire les risques de l'IA avancée, si une IA beaucoup plus puissante est créée. La création d'une IA sûre peut être facile, mais nous devons planifier des situations plus difficiles. </p><p>Anthropic adopte une approche fondée sur des données probantes pour la sécurité de l'IA. Nous travaillons à: mieux comprendre comment l'IA apprend et applique les connaissances dans le monde réel; développer des moyens de superviser et de vérifier l'IA à grande échelle; construire une IA transparente et interprétable; former l'IA à suivre des étapes sûres plutôt qu'à poursuivre simplement des objectifs; analyser comment l'IA pourrait échouer dangereusement et l'en empêcher; évaluer les effets de l'IA sur la société pour guider les règles et la recherche.</p><p>En abordant la sécurité de l'IA de plusieurs manières, nous espérons réussir dans différents scénarios. Nous nous adapterons à mesure que nous en apprendrons davantage. Les objectifs sont: se préparer aux systèmes avancés; minimiser les risques de catastrophe si le progrès dépasse les garde-fous. Bien que l'IA d'aujourd'hui ne soit pas une menace immédiate, nous nous attaquons de manière proactive aux défis posés par les progrès rapides.</p><p>Exemples de notre approche: Étude de la façon dont les modèles appliquent l'apprentissage à des environnements complexes. Création de méthodes de supervision et d'examen pour l'IA. Construire des systèmes transparents et interprétables. Concentrer les modèles sur des étapes sûres plutôt que sur des objectifs. Identifier les échecs dangereux et les contre-mesures. Évaluer les impacts sociétaux pour informer les décideurs politiques et les chercheurs.</p>  <p>Un travail réfléchi et fondé sur des preuves aujourd'hui peut aider à faire en sorte que les progrès à long terme de l'IA soient alignés, bénéfiques et dignes de confiance. Nous visons une IA de plus en plus puissante mais intrinsèquement sûre et éthique. En abordant systématiquement et de manière responsable la sécurité, nous espérons réduire les risques et construire un avenir positif avec la technologie avancée. Mais nous connaissons les défis à venir et pensons que la préparation et les progrès continus sont essentiels. </p><h4>Notes de bas de page</h4> <ol><li> Les progrès dans le développement de nouvelles méthodes d'entraînement de l'IA (progrès algorithmiques) sont difficiles à mesurer mais semblent exponentiels et plus rapides que la loi de Moore. Pour estimer le progrès général de l'IA, multiplier la croissance exponentielle du financement, de la puissance de calcul et des progrès algorithmiques. </li> <li> Les lois d'échelle ont justifié le travail, mais un autre objectif était de pivoter vers une IA capable de lire/écrire pour entraîner/tester facilement une IA capable de considérer les valeurs humaines. </li><li> Deviner les progrès de l'IA grâce à plus de calcul est imparfait et nécessite du jugement. GPT-3 provenait principalement de 250 fois plus de calcul que GPT-2. GPT-3 à l'état de l'art en 2023 pourrait nécessiter 50 fois plus. Dans 5 ans, 1000 fois plus de calcul pourrait entraîner les plus grands modèles, selon les tendances. Si les lois d'échelle tiennent, les progrès dépasseraient de loin GPT-2 à GPT-3. Chez Anthropic, nous connaissons bien ces systèmes - ce bond pourrait atteindre des performances de niveau humain dans la plupart des tâches. Cela utilise une intuition informée, donc imparfaite, mais des faits comme: (i) la différence de calcul dans les systèmes; (ii) la différence de performance; (iii) les lois d'échelle pour projeter les futurs systèmes; (iv) les tendances en matière de coût/dépenses de calcul; suggèrent ensemble plus de 10% de chances d'avoir une IA de niveau humain général dans 10 ans. Cette analyse grossière ignore les progrès algorithmiques et n'est pas sûre des nombres de calcul, mais la plupart des désaccords consistent à intuitiver les progrès futurs à partir de sauts de calcul équivalents. </li><li> Par exemple, beaucoup ont supposé que les minima locaux empêcheraient l'apprentissage des réseaux de neurones, tandis que des aspects tels que les exemples adversaires généralisés ont été des surprises. </li><li> Une recherche efficace sur la sécurité des grands modèles nécessite plus qu'un accès nominal (par exemple API) - pour faire de l'interprétabilité, du réglage fin, de l'apprentissage par renforcement, nous devons développer l'IA en interne chez Anthropic.</li></ol><p>En résumé, il est difficile de mesurer ou de prédire avec précision les progrès de l'IA avancée. Mais il y a des raisons de croire que des capacités importantes de niveau humain pourraient émerger dans les 10 prochaines années, sur la base de:-Augmentations exponentielles du financement, de la puissance de calcul et des progrès dans le développement de nouvelles techniques d'entraînement-Lois d'échelle suggérant de grands bonds en capacité grâce à plus de calcul, comme dans GPT-2 à GPT-3-Intuition informée chez Anthropic, où nous construisons et étudions directement ces systèmes-Faits comme les tendances en matière de coût/dépenses de calcul et les différences permettant GPT-3 pointent vers de bonnes chances de progrès majeurs.Cependant, l'extrapolation est imparfaite. Des surprises émergent, comme les exemples adversaires. Et la recherche sur la </p>