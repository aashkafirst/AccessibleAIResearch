<h1>Lois d'échelle et interprétabilité de l'apprentissage à partir de données répétées</h1><h2>En termes simples en français</h2><h4>Résumé</h4><p>De récents modèles linguistiques de grande envergure ont été entraînés sur d'énormes ensembles de données, mais ils ont également souvent utilisé des données répétées, soit intentionnellement dans le but de privilégier des données de meilleure qualité, soit involontairement en raison de l'imperfection de la déduplication des données, ce qui expose le modèle à des données répétées au niveau de la phrase, du paragraphe ou du document. Certains travaux ont rapporté des effets négatifs importants sur les performances dus à ces données répétées. Dans cet article, nous tentons d'étudier systématiquement les données répétées et de comprendre leurs effets de manière mécanistique. Pour cela, nous entraînons une famille de modèles où la plupart des données sont uniques, mais une petite fraction est répétée plusieurs fois. Nous observons un phénomène de double descente prononcé, dans lequel les données répétées peuvent entraîner une augmentation de la perte de test à mi-parcours de l'entraînement. Une plage prévisible de fréquence de répétition entraîne une dégradation surprenante des performances. Par exemple, la performance d'un modèle de 800 millions de paramètres peut être réduite à celle d'un modèle deux fois plus petit (400 millions de paramètres) en répétant 0,1% des données 100 fois, malgré le fait que les 90% restants des jetons d'entraînement soient uniques. Nous soupçonnons qu'il existe une plage au milieu où les données peuvent être mémorisées, ce qui consomme une grande partie de la capacité du modèle, et c'est peut-être là que le pic de dégradation se produit. Enfin, nous relions ces observations aux travaux récents sur l'interprétabilité mécanistique, en tentant d'analyser en détail les calculs effectués par le modèle, en montrant que la répétition des données endommage de manière disproportionnée la copie et les structures internes associées à la généralisation, comme les têtes d'induction, ce qui fournit un mécanisme possible pour le passage de la généralisation à la mémorisation. Dans l'ensemble, ces résultats fournissent une hypothèse expliquant pourquoi la répétition d'une fraction relativement faible de données dans les grands modèles linguistiques peut entraîner des dommages disproportionnés aux performances.</p><h4>Auteurs</h4><p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Jared Kaplan<br/></p>