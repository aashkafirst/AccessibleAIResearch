<h1>Scaling Laws and Interpretability of Learning from Repeated Data</h1><h2>In Layman English</h2><h4>Abstract</h4><p>Recent large language models have been trained on vast datasets. However, some of the data used for training is repeated either intentionally to prioritize high-quality data or unintentionally due to imperfect data deduplication. This exposure to repeated data at different levels (sentence, paragraph, or document) has been found to negatively impact performance. In this paper, we investigate the effects of repeated data systematically. We train a group of models with mostly unique data but include a small fraction of data that is repeated multiple times. Our findings reveal a phenomenon called double descent, where the test loss increases during training due to repeated data. Surprisingly, a certain range of repetition frequency severely degrades performance. For example, by repeating only 0.1% of the data 100 times, the performance of an 800M parameter model can be reduced to that of a 400M parameter model, even though the remaining 90% of training tokens are unique. We suspect that there is a middle range where the model memorizes the data, consuming a significant portion of its capacity, leading to the peak degradation. Additionally, we connect these findings to recent interpretability research, which aims to understand the model's computations. We demonstrate that data repetition disproportionately damages copying and internal structures related to generalization, such as induction heads. This provides a possible explanation for the shift from generalization to memorization. Overall, our results suggest a hypothesis that repeating a relatively small fraction of data in large language models can significantly harm their performance.</p><h4>Authors</h4><p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Jared Kaplan</p>